{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus # sample text for performing tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>wrong isi follows example mohammed quran exactly</td>\n",
       "      <td>['wrong', 'isi', 'follows', 'example', 'mohamm...</td>\n",
       "      <td>['wrong', 'isi', 'follow', 'exampl', 'moham', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@SirajZarook @OdiniaInvictus @BilalIGhumman @I...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>good muslim good despite bad religion</td>\n",
       "      <td>['good', 'muslim', 'good', 'despite', 'bad', '...</td>\n",
       "      <td>['good', 'muslim', 'good', 'despit', 'bad', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@scamp_faridxx @AbuAlbaraaSham Yeah, it's call...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>yeah called caring human life idiot something ...</td>\n",
       "      <td>['yeah', 'called', 'caring', 'human', 'life', ...</td>\n",
       "      <td>['yeah', 'call', 'care', 'human', 'life', 'idi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>@Asadumarfans You are a Muslim. You are brain ...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>muslim brain dead repeat others said million time</td>\n",
       "      <td>['muslim', 'brain', 'dead', 'repeat', 'others'...</td>\n",
       "      <td>['muslim', 'brain', 'dead', 'repeat', 'other',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@harmlesstree2 @MaxBlumenthal If you want to u...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>want understand lie muslim living peace jew re...</td>\n",
       "      <td>['want', 'understand', 'lie', 'muslim', 'livin...</td>\n",
       "      <td>['want', 'understand', 'lie', 'muslim', 'live'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0           0   \n",
       "1             1             1           1   \n",
       "2             2             2           2   \n",
       "3             3             3           3   \n",
       "4             4             4           4   \n",
       "\n",
       "                                                text annotation  oh_label  \\\n",
       "0  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0   \n",
       "1  @SirajZarook @OdiniaInvictus @BilalIGhumman @I...     racism       1.0   \n",
       "2  @scamp_faridxx @AbuAlbaraaSham Yeah, it's call...     racism       1.0   \n",
       "3  @Asadumarfans You are a Muslim. You are brain ...     racism       1.0   \n",
       "4  @harmlesstree2 @MaxBlumenthal If you want to u...     racism       1.0   \n",
       "\n",
       "   cat_enc ed_label_0 ed_label_1 hashtags  \\\n",
       "0        1                             []   \n",
       "1        1                             []   \n",
       "2        1                             []   \n",
       "3        1                             []   \n",
       "4        1                             []   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0   wrong isi follows example mohammed quran exactly   \n",
       "1              good muslim good despite bad religion   \n",
       "2  yeah called caring human life idiot something ...   \n",
       "3  muslim brain dead repeat others said million time   \n",
       "4  want understand lie muslim living peace jew re...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['wrong', 'isi', 'follows', 'example', 'mohamm...   \n",
       "1  ['good', 'muslim', 'good', 'despite', 'bad', '...   \n",
       "2  ['yeah', 'called', 'caring', 'human', 'life', ...   \n",
       "3  ['muslim', 'brain', 'dead', 'repeat', 'others'...   \n",
       "4  ['want', 'understand', 'lie', 'muslim', 'livin...   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  ['wrong', 'isi', 'follow', 'exampl', 'moham', ...  \n",
       "1  ['good', 'muslim', 'good', 'despit', 'bad', 'r...  \n",
       "2  ['yeah', 'call', 'care', 'human', 'life', 'idi...  \n",
       "3  ['muslim', 'brain', 'dead', 'repeat', 'other',...  \n",
       "4  ['want', 'understand', 'lie', 'muslim', 'live'...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing twitter dataset\n",
    "\n",
    "twitter_df = pd.read_csv('data/twitter_all_data.csv',index_col=False)\n",
    "twitter_df=twitter_df.fillna(\"\")\n",
    "\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.2, Unnamed: 0.1, Unnamed: 0, text, annotation, oh_label, cat_enc, ed_label_0, ed_label_1, hashtags, tokenized, tokenized_text, stemmed_tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['oh_label']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wrong',\n",
       "  'isi',\n",
       "  'follows',\n",
       "  'example',\n",
       "  'mohammed',\n",
       "  'quran',\n",
       "  'exactly',\n",
       "  'good',\n",
       "  'muslim',\n",
       "  'good',\n",
       "  'despite',\n",
       "  'bad',\n",
       "  'religion',\n",
       "  'yeah',\n",
       "  'called',\n",
       "  'caring',\n",
       "  'human',\n",
       "  'life',\n",
       "  'idiot',\n",
       "  'something',\n",
       "  'genocidal',\n",
       "  'daesh',\n",
       "  'would',\n",
       "  'nt',\n",
       "  'understand',\n",
       "  'muslim',\n",
       "  'brain',\n",
       "  'dead',\n",
       "  'repeat',\n",
       "  'others',\n",
       "  'said',\n",
       "  'million',\n",
       "  'time',\n",
       "  'want',\n",
       "  'understand',\n",
       "  'lie',\n",
       "  'muslim',\n",
       "  'living',\n",
       "  'peace',\n",
       "  'jew',\n",
       "  'read',\n",
       "  'ibn',\n",
       "  'warraq',\n",
       "  'total',\n",
       "  'liar',\n",
       "  'like',\n",
       "  'pedophile',\n",
       "  'prophet',\n",
       "  'un',\n",
       "  'soldier',\n",
       "  'burn',\n",
       "  'people',\n",
       "  'alive',\n",
       "  'daesh',\n",
       "  'shia',\n",
       "  'militia',\n",
       "  'driven',\n",
       "  'religion',\n",
       "  'hatred',\n",
       "  'bigotry',\n",
       "  'freedom',\n",
       "  'tikrit',\n",
       "  'regardless',\n",
       "  'muslim',\n",
       "  'world',\n",
       "  'ever',\n",
       "  'produced',\n",
       "  'anything',\n",
       "  'tyrant',\n",
       "  'dictator',\n",
       "  'fascist',\n",
       "  'fanatic',\n",
       "  'would',\n",
       "  'support',\n",
       "  'islam',\n",
       "  'answer',\n",
       "  'anything',\n",
       "  'pretend',\n",
       "  'answer',\n",
       "  'illogical',\n",
       "  'delusional',\n",
       "  'superstition',\n",
       "  'attacking',\n",
       "  'everyone',\n",
       "  'follows',\n",
       "  'religious',\n",
       "  'cult',\n",
       "  'hated',\n",
       "  'murder',\n",
       "  'like',\n",
       "  'islam',\n",
       "  'liar',\n",
       "  'beheaded',\n",
       "  'jewish',\n",
       "  'men',\n",
       "  'one',\n",
       "  'day',\n",
       "  'sold',\n",
       "  'woman',\n",
       "  'child',\n",
       "  'slavery',\n",
       "  'islam',\n",
       "  'race',\n",
       "  'microbrain',\n",
       "  'death',\n",
       "  'cult',\n",
       "  'stupidist',\n",
       "  'argument',\n",
       "  'one',\n",
       "  'rationalizing',\n",
       "  'barbarity',\n",
       "  'islam',\n",
       "  'rofl',\n",
       "  'jew',\n",
       "  'used',\n",
       "  'live',\n",
       "  'arabian',\n",
       "  'peninsula',\n",
       "  'committing',\n",
       "  'genocide',\n",
       "  'rt',\n",
       "  'good',\n",
       "  'morning',\n",
       "  'pissed',\n",
       "  'american',\n",
       "  'patriot',\n",
       "  'cop',\n",
       "  'danger',\n",
       "  'religion',\n",
       "  'islam',\n",
       "  'built',\n",
       "  'apartheid',\n",
       "  'ethnic',\n",
       "  'cleansing',\n",
       "  'rt',\n",
       "  'isi',\n",
       "  'trying',\n",
       "  'stop',\n",
       "  'people',\n",
       "  'fleeing',\n",
       "  'talabyad',\n",
       "  'gre',\n",
       "  'spi',\n",
       "  'lie',\n",
       "  'tht',\n",
       "  'thy',\n",
       "  'captured',\n",
       "  'rocket',\n",
       "  'voice',\n",
       "  'hit',\n",
       "  'near',\n",
       "  'choose',\n",
       "  'define',\n",
       "  'quran',\n",
       "  'follower',\n",
       "  'extremist',\n",
       "  'care',\n",
       "  'revised',\n",
       "  'million',\n",
       "  'time',\n",
       "  'barbarity',\n",
       "  'islam',\n",
       "  'mohammed',\n",
       "  'people',\n",
       "  'murdered',\n",
       "  'disagreed',\n",
       "  'raped',\n",
       "  'woman',\n",
       "  'married',\n",
       "  'year',\n",
       "  'old',\n",
       "  'beheaded',\n",
       "  'jew',\n",
       "  'salon',\n",
       "  'try',\n",
       "  'shut',\n",
       "  'free',\n",
       "  'speech',\n",
       "  'shut',\n",
       "  'people',\n",
       "  'playing',\n",
       "  'race',\n",
       "  'card',\n",
       "  'ideaology',\n",
       "  'let',\n",
       "  'people',\n",
       "  'read',\n",
       "  'quran',\n",
       "  'see',\n",
       "  'come',\n",
       "  'hate',\n",
       "  'true',\n",
       "  'islam',\n",
       "  'corrupt',\n",
       "  'beginning',\n",
       "  'feed',\n",
       "  'little',\n",
       "  'piece',\n",
       "  'taqiyya',\n",
       "  'rt',\n",
       "  'article',\n",
       "  'islamist',\n",
       "  'welfare',\n",
       "  'paid',\n",
       "  'plot',\n",
       "  'west',\n",
       "  'demise',\n",
       "  'idiot',\n",
       "  'history',\n",
       "  'would',\n",
       "  'say',\n",
       "  'thatyou',\n",
       "  'never',\n",
       "  'studied',\n",
       "  'barbarity',\n",
       "  'muslim',\n",
       "  'entire',\n",
       "  'time',\n",
       "  'mean',\n",
       "  'send',\n",
       "  'muslim',\n",
       "  'back',\n",
       "  'middle',\n",
       "  'east',\n",
       "  'turn',\n",
       "  'europe',\n",
       "  'sewer',\n",
       "  'mohammed',\n",
       "  'first',\n",
       "  'wife',\n",
       "  'katjia',\n",
       "  'wealthy',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'lying',\n",
       "  'muslim',\n",
       "  'extermination',\n",
       "  'jew',\n",
       "  'rt',\n",
       "  'woman',\n",
       "  'subjected',\n",
       "  'brutal',\n",
       "  'abnormal',\n",
       "  'sex',\n",
       "  'act',\n",
       "  'marrying',\n",
       "  'isi',\n",
       "  'militant',\n",
       "  'big',\n",
       "  'lie',\n",
       "  'advocate',\n",
       "  'tolerance',\n",
       "  'coexistence',\n",
       "  'advocate',\n",
       "  'hatred',\n",
       "  'murder',\n",
       "  'try',\n",
       "  'get',\n",
       "  'education',\n",
       "  'repeat',\n",
       "  'lie',\n",
       "  'stupidity',\n",
       "  'get',\n",
       "  'imam',\n",
       "  'rt',\n",
       "  'people',\n",
       "  'work',\n",
       "  'hard',\n",
       "  'make',\n",
       "  'world',\n",
       "  'better',\n",
       "  'place',\n",
       "  'others',\n",
       "  'work',\n",
       "  'hard',\n",
       "  'book',\n",
       "  'drink',\n",
       "  'sex',\n",
       "  'party',\n",
       "  'next',\n",
       "  'door',\n",
       "  'pakistan',\n",
       "  'muslim',\n",
       "  'still',\n",
       "  'murder',\n",
       "  'christian',\n",
       "  'hindu',\n",
       "  'regularly',\n",
       "  'read',\n",
       "  'entire',\n",
       "  'quran',\n",
       "  'much',\n",
       "  'hadith',\n",
       "  'know',\n",
       "  'islam',\n",
       "  'muslim',\n",
       "  'curse',\n",
       "  'mankind',\n",
       "  'comparing',\n",
       "  'thousand',\n",
       "  'jewish',\n",
       "  'extremist',\n",
       "  'million',\n",
       "  'muslim',\n",
       "  'extremist',\n",
       "  'religion',\n",
       "  'ready',\n",
       "  'murder',\n",
       "  'apostate',\n",
       "  'moderate',\n",
       "  'muslim',\n",
       "  'woman',\n",
       "  'convert',\n",
       "  'islam',\n",
       "  'marry',\n",
       "  'muslim',\n",
       "  'men',\n",
       "  'men',\n",
       "  'convert',\n",
       "  'prison',\n",
       "  'actual',\n",
       "  'spirituality',\n",
       "  'buddhism',\n",
       "  'tao',\n",
       "  'zero',\n",
       "  'spirituality',\n",
       "  'islam',\n",
       "  'kiss',\n",
       "  'as',\n",
       "  'read',\n",
       "  'book',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'written',\n",
       "  'promote',\n",
       "  'fascist',\n",
       "  'murdering',\n",
       "  'genocide',\n",
       "  'humanity',\n",
       "  'another',\n",
       "  'race',\n",
       "  'baiting',\n",
       "  'idiot',\n",
       "  'brown',\n",
       "  'men',\n",
       "  'would',\n",
       "  'hate',\n",
       "  'hinduism',\n",
       "  'buddhism',\n",
       "  'tao',\n",
       "  'hate',\n",
       "  'islam',\n",
       "  'hell',\n",
       "  'palestinian',\n",
       "  'muslim',\n",
       "  'asshole',\n",
       "  'like',\n",
       "  'clearly',\n",
       "  'state',\n",
       "  'intention',\n",
       "  'religious',\n",
       "  'jihad',\n",
       "  'jew',\n",
       "  'tell',\n",
       "  'shit',\n",
       "  'sucking',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'like',\n",
       "  'recognize',\n",
       "  'history',\n",
       "  'crawled',\n",
       "  'cuntyou',\n",
       "  'think',\n",
       "  'photoshop',\n",
       "  'truth',\n",
       "  'machin',\n",
       "  'idiotic',\n",
       "  'never',\n",
       "  'million',\n",
       "  'native',\n",
       "  'american',\n",
       "  'died',\n",
       "  'disease',\n",
       "  'prophet',\n",
       "  'mohammed',\n",
       "  'started',\n",
       "  'hate',\n",
       "  'fest',\n",
       "  'jew',\n",
       "  'driving',\n",
       "  'three',\n",
       "  'jewish',\n",
       "  'tribe',\n",
       "  'medina',\n",
       "  'jew',\n",
       "  'completely',\n",
       "  'perished',\n",
       "  'much',\n",
       "  'arabian',\n",
       "  'kind',\n",
       "  'thing',\n",
       "  'prophet',\n",
       "  'mohammed',\n",
       "  'mohammed',\n",
       "  'never',\n",
       "  'looked',\n",
       "  'knowledge',\n",
       "  'believed',\n",
       "  'jinni',\n",
       "  'illiterate',\n",
       "  'believed',\n",
       "  'dog',\n",
       "  'ere',\n",
       "  'devil',\n",
       "  'meantime',\n",
       "  'palestinian',\n",
       "  'fucker',\n",
       "  'beheading',\n",
       "  'gay',\n",
       "  'honor',\n",
       "  'killing',\n",
       "  'that',\n",
       "  'muslim',\n",
       "  'react',\n",
       "  'traffic',\n",
       "  'light',\n",
       "  'turn',\n",
       "  'red',\n",
       "  'unfortunately',\n",
       "  'real',\n",
       "  'islam',\n",
       "  'defined',\n",
       "  'quran',\n",
       "  'hadith',\n",
       "  'backwards',\n",
       "  'inhuman',\n",
       "  'even',\n",
       "  'muslim',\n",
       "  'yes',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'murdering',\n",
       "  'christian',\n",
       "  'africa',\n",
       "  'decade',\n",
       "  'mohammed',\n",
       "  'imperialist',\n",
       "  'led',\n",
       "  'major',\n",
       "  'military',\n",
       "  'expedition',\n",
       "  'isi',\n",
       "  'imperialist',\n",
       "  'rt',\n",
       "  'simple',\n",
       "  'question',\n",
       "  'ask',\n",
       "  'complaining',\n",
       "  'muslim',\n",
       "  'etc',\n",
       "  'integrate',\n",
       "  'live',\n",
       "  'saying',\n",
       "  'islam',\n",
       "  'way',\n",
       "  'life',\n",
       "  'mean',\n",
       "  'tell',\n",
       "  'can',\n",
       "  'not',\n",
       "  'democracy',\n",
       "  'must',\n",
       "  'instead',\n",
       "  'follow',\n",
       "  'pedophile',\n",
       "  'skill',\n",
       "  'except',\n",
       "  'jihad',\n",
       "  'rt',\n",
       "  'muhammad',\n",
       "  'inventor',\n",
       "  'thighing',\n",
       "  'started',\n",
       "  'molesting',\n",
       "  'aisha',\n",
       "  'year',\n",
       "  'old',\n",
       "  'cc',\n",
       "  'absolute',\n",
       "  'truth',\n",
       "  'verify',\n",
       "  'hundred',\n",
       "  'source',\n",
       "  'anti',\n",
       "  'semitism',\n",
       "  'scripted',\n",
       "  'propaganda',\n",
       "  'truth',\n",
       "  'muslim',\n",
       "  'walking',\n",
       "  'away',\n",
       "  'islam',\n",
       "  'people',\n",
       "  'converting',\n",
       "  'education',\n",
       "  'kill',\n",
       "  'say',\n",
       "  'must',\n",
       "  'murdering',\n",
       "  'fascist',\n",
       "  'follow',\n",
       "  'book',\n",
       "  'right',\n",
       "  'living',\n",
       "  'severe',\n",
       "  'delusion',\n",
       "  'reason',\n",
       "  'mohammed',\n",
       "  'hated',\n",
       "  'jew',\n",
       "  'willing',\n",
       "  'convert',\n",
       "  'million',\n",
       "  'african',\n",
       "  'murdered',\n",
       "  'islam',\n",
       "  've',\n",
       "  'seen',\n",
       "  'microbrain',\n",
       "  'told',\n",
       "  'every',\n",
       "  'verse',\n",
       "  'tao',\n",
       "  'te',\n",
       "  'ching',\n",
       "  'superior',\n",
       "  'every',\n",
       "  'verse',\n",
       "  'quran',\n",
       "  'brain',\n",
       "  'would',\n",
       "  'revolted',\n",
       "  'islam',\n",
       "  'isi',\n",
       "  'different',\n",
       "  'max',\n",
       "  'friend',\n",
       "  'hamas',\n",
       "  'isi',\n",
       "  'throw',\n",
       "  'gay',\n",
       "  'rooftop',\n",
       "  'hamas',\n",
       "  'beheads',\n",
       "  'gay',\n",
       "  'lesson',\n",
       "  'world',\n",
       "  'learned',\n",
       "  'islam',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'brutal',\n",
       "  'dictator',\n",
       "  'replaced',\n",
       "  'brutal',\n",
       "  'islamist',\n",
       "  'that',\n",
       "  'complete',\n",
       "  'lie',\n",
       "  'murdered',\n",
       "  'unarmed',\n",
       "  'ezidi',\n",
       "  'civilian',\n",
       "  'done',\n",
       "  'nothing',\n",
       "  'enslaved',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'nothing',\n",
       "  'freeze',\n",
       "  'status',\n",
       "  'woman',\n",
       "  'century',\n",
       "  'denouncing',\n",
       "  'terrorism',\n",
       "  'violence',\n",
       "  'mean',\n",
       "  'denouncing',\n",
       "  'heart',\n",
       "  'islam',\n",
       "  'amazing',\n",
       "  'often',\n",
       "  'allah',\n",
       "  'revelation',\n",
       "  'met',\n",
       "  'mohammed',\n",
       "  'desire',\n",
       "  'one',\n",
       "  'became',\n",
       "  'suspicious',\n",
       "  'except',\n",
       "  'aisha',\n",
       "  'muslim',\n",
       "  'exterminated',\n",
       "  'christian',\n",
       "  'jew',\n",
       "  'continue',\n",
       "  'today',\n",
       "  'un',\n",
       "  'resolution',\n",
       "  'old',\n",
       "  'lie',\n",
       "  'property',\n",
       "  'right',\n",
       "  'arab',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'furthermore',\n",
       "  'many',\n",
       "  'muslim',\n",
       "  'country',\n",
       "  'even',\n",
       "  'record',\n",
       "  'system',\n",
       "  'account',\n",
       "  'murder',\n",
       "  'proving',\n",
       "  'muslim',\n",
       "  'liar',\n",
       "  'muslim',\n",
       "  'student',\n",
       "  'usc',\n",
       "  'hadith',\n",
       "  'database',\n",
       "  'muslim',\n",
       "  'virtually',\n",
       "  'killing',\n",
       "  'example',\n",
       "  'civilian',\n",
       "  'killed',\n",
       "  'afghanistan',\n",
       "  'killed',\n",
       "  'taliban',\n",
       "  'follower',\n",
       "  'religion',\n",
       "  'give',\n",
       "  'shit',\n",
       "  'prophet',\n",
       "  'religion',\n",
       "  'example',\n",
       "  'always',\n",
       "  'rt',\n",
       "  'spent',\n",
       "  'morning',\n",
       "  'board',\n",
       "  'election',\n",
       "  'getting',\n",
       "  'map',\n",
       "  'data',\n",
       "  'start',\n",
       "  'registering',\n",
       "  'every',\n",
       "  'black',\n",
       "  'person',\n",
       "  'ht',\n",
       "  'could',\n",
       "  'care',\n",
       "  'le',\n",
       "  'much',\n",
       "  'love',\n",
       "  'fraud',\n",
       "  'right',\n",
       "  'think',\n",
       "  'way',\n",
       "  'want',\n",
       "  'express',\n",
       "  'discovery',\n",
       "  'made',\n",
       "  'despite',\n",
       "  'islam',\n",
       "  'islam',\n",
       "  'contributed',\n",
       "  'nothing',\n",
       "  'mankind',\n",
       "  'year',\n",
       "  'muslim',\n",
       "  'kill',\n",
       "  'time',\n",
       "  'many',\n",
       "  'innocent',\n",
       "  'non',\n",
       "  'muslim',\n",
       "  'every',\n",
       "  'day',\n",
       "  'care',\n",
       "  'earthly',\n",
       "  'tyrant',\n",
       "  'also',\n",
       "  'egotistical',\n",
       "  'want',\n",
       "  'everyone',\n",
       "  'admire',\n",
       "  'bow',\n",
       "  'quran',\n",
       "  'writer',\n",
       "  'imagined',\n",
       "  'god',\n",
       "  'course',\n",
       "  'campaigning',\n",
       "  'destruction',\n",
       "  'israeli',\n",
       "  'racist',\n",
       "  'anti',\n",
       "  'semite',\n",
       "  'asshole',\n",
       "  'rt',\n",
       "  'islam',\n",
       "  'war',\n",
       "  'woman',\n",
       "  'continues',\n",
       "  'islam',\n",
       "  'continues',\n",
       "  'show',\n",
       "  'value',\n",
       "  'woman',\n",
       "  'factual',\n",
       "  'evidence',\n",
       "  'never',\n",
       "  'million',\n",
       "  'native',\n",
       "  'american',\n",
       "  'indian',\n",
       "  'north',\n",
       "  'american',\n",
       "  'continent',\n",
       "  'rt',\n",
       "  'uk',\n",
       "  'like',\n",
       "  'idiot',\n",
       "  'give',\n",
       "  'jihadis',\n",
       "  'rehabilitation',\n",
       "  'jordan',\n",
       "  'execute',\n",
       "  'joke',\n",
       "  'support',\n",
       "  'tyrant',\n",
       "  'islam',\n",
       "  'produce',\n",
       "  'nothing',\n",
       "  'tyrant',\n",
       "  'caliph',\n",
       "  'sultan',\n",
       "  'tyrant',\n",
       "  'muslim',\n",
       "  'robbed',\n",
       "  'wealthy',\n",
       "  'merchant',\n",
       "  'saudi',\n",
       "  'cleric',\n",
       "  'sun',\n",
       "  'revolves',\n",
       "  'around',\n",
       "  'earth',\n",
       "  'understand',\n",
       "  'madrassa',\n",
       "  'graduate',\n",
       "  'skill',\n",
       "  'jihad',\n",
       "  'islam',\n",
       "  'created',\n",
       "  'monster',\n",
       "  'islam',\n",
       "  'creating',\n",
       "  'year',\n",
       "  'allah',\n",
       "  'word',\n",
       "  'filth',\n",
       "  'hatred',\n",
       "  'phony',\n",
       "  'seen',\n",
       "  'jew',\n",
       "  'go',\n",
       "  'killing',\n",
       "  'rampage',\n",
       "  'holocaust',\n",
       "  'denier',\n",
       "  'violence',\n",
       "  'islam',\n",
       "  'answer',\n",
       "  'everything',\n",
       "  'say',\n",
       "  'drink',\n",
       "  'wine',\n",
       "  'heaven',\n",
       "  'islamic',\n",
       "  'heaven',\n",
       "  'basically',\n",
       "  'drunken',\n",
       "  'whore',\n",
       "  'house',\n",
       "  'spiritual',\n",
       "  'lol',\n",
       "  'claim',\n",
       "  'oh',\n",
       "  'insulted',\n",
       "  'religion',\n",
       "  'kill',\n",
       "  'child',\n",
       "  'grow',\n",
       "  'fairy',\n",
       "  'tale',\n",
       "  'muslim',\n",
       "  'never',\n",
       "  'grow',\n",
       "  'santa',\n",
       "  'white',\n",
       "  'entire',\n",
       "  'nation',\n",
       "  'saudi',\n",
       "  'punnished',\n",
       "  'since',\n",
       "  'allow',\n",
       "  'church',\n",
       "  'bible',\n",
       "  'tell',\n",
       "  'something',\n",
       "  'isi',\n",
       "  'mohammed',\n",
       "  'freedom',\n",
       "  'impose',\n",
       "  'religion',\n",
       "  'others',\n",
       "  'islam',\n",
       "  'yes',\n",
       "  'might',\n",
       "  'power',\n",
       "  'imperialism',\n",
       "  'zero',\n",
       "  'spirituality',\n",
       "  'soon',\n",
       "  'world',\n",
       "  'understand',\n",
       "  'see',\n",
       "  'burning',\n",
       "  'civilian',\n",
       "  'picture',\n",
       "  'even',\n",
       "  'burning',\n",
       "  'house',\n",
       "  'million',\n",
       "  'muslim',\n",
       "  'extremist',\n",
       "  'matter',\n",
       "  'desperately',\n",
       "  'race',\n",
       "  'baiting',\n",
       "  'trash',\n",
       "  'like',\n",
       "  'want',\n",
       "  'islam',\n",
       "  'race',\n",
       "  'like',\n",
       "  'said',\n",
       "  'stupid',\n",
       "  'comment',\n",
       "  'seek',\n",
       "  'hide',\n",
       "  'vileness',\n",
       "  'islam',\n",
       "  'rt',\n",
       "  'muslim',\n",
       "  'protest',\n",
       "  'complete',\n",
       "  'failure',\n",
       "  'aussie',\n",
       "  'want',\n",
       "  'islam',\n",
       "  'suit',\n",
       "  'way',\n",
       "  'life',\n",
       "  'every',\n",
       "  'jew',\n",
       "  'like',\n",
       "  'thousand',\n",
       "  'muslim',\n",
       "  'like',\n",
       "  'problem',\n",
       "  'far',\n",
       "  'knowledge',\n",
       "  'islam',\n",
       "  'mind',\n",
       "  'shrunken',\n",
       "  'islam',\n",
       "  'execution',\n",
       "  'that',\n",
       "  'one',\n",
       "  'tenth',\n",
       "  'day',\n",
       "  'worth',\n",
       "  'islamically',\n",
       "  'correct',\n",
       "  'isi',\n",
       "  'justifying',\n",
       "  'christianity',\n",
       "  'microbrain',\n",
       "  'religion',\n",
       "  'hiding',\n",
       "  'islamic',\n",
       "  'barbarity',\n",
       "  'billion',\n",
       "  'muslim',\n",
       "  'idiot',\n",
       "  'size',\n",
       "  'population',\n",
       "  'contribute',\n",
       "  'virtually',\n",
       "  'nothing',\n",
       "  'every',\n",
       "  'muslim',\n",
       "  'state',\n",
       "  'nasty',\n",
       "  'place',\n",
       "  'muslim',\n",
       "  'blumenthal',\n",
       "  'ever',\n",
       "  'written',\n",
       "  'ceaseless',\n",
       "  'muslim',\n",
       "  'violence',\n",
       "  'hindu',\n",
       "  'bangladesh',\n",
       "  'practicing',\n",
       "  'islam',\n",
       "  'vomiting',\n",
       "  'jew',\n",
       "  'christian',\n",
       "  'non',\n",
       "  'believer',\n",
       "  'check',\n",
       "  'code',\n",
       "  'umar',\n",
       "  'three',\n",
       "  'translation',\n",
       "  'quran',\n",
       "  'microbrain',\n",
       "  'know',\n",
       "  'far',\n",
       "  'islam',\n",
       "  'idiot',\n",
       "  'like',\n",
       "  'making',\n",
       "  'declaration',\n",
       "  'contact',\n",
       "  'reality',\n",
       "  'islam',\n",
       "  'inhuman',\n",
       "  'must',\n",
       "  'outlawed',\n",
       "  'rt',\n",
       "  'prophet',\n",
       "  'honor',\n",
       "  'rape',\n",
       "  'beheading',\n",
       "  'genocide',\n",
       "  'honor',\n",
       "  'muhammad',\n",
       "  'deserve',\n",
       "  'respect',\n",
       "  'quran',\n",
       "  'would',\n",
       "  'good',\n",
       "  'example',\n",
       "  'terrorism',\n",
       "  'phobia',\n",
       "  'irrational',\n",
       "  'hatred',\n",
       "  'thing',\n",
       "  'phobia',\n",
       "  'hatred',\n",
       "  'rational',\n",
       "  'islam',\n",
       "  'religion',\n",
       "  'zero',\n",
       "  'spiritual',\n",
       "  'content',\n",
       "  'tell',\n",
       "  'exactly',\n",
       "  'wipe',\n",
       "  'as',\n",
       "  'furthermore',\n",
       "  'islam',\n",
       "  'demand',\n",
       "  'woman',\n",
       "  'locked',\n",
       "  'home',\n",
       "  'covered',\n",
       "  'stop',\n",
       "  'lying',\n",
       "  ...]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "tweets = \" \".join(tw for tw in twitter_df.tokenized)\n",
    "all_sentences = nltk.sent_tokenize(tweets)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "all_words[:10]\n",
    "#stemmed_tokens = pd.Series(df['stemmed_tokens']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [wrong, isi, follows, example, mohammed, quran...\n",
      "1         [good, muslim, good, despite, bad, religion]\n",
      "2    [yeah, called, caring, human, life, idiot, som...\n",
      "3    [muslim, brain, dead, repeat, others, said, mi...\n",
      "4    [want, understand, lie, muslim, living, peace,...\n",
      "5    [total, liar, like, pedophile, prophet, un, so...\n",
      "6    [daesh, shia, militia, driven, religion, hatre...\n",
      "7    [muslim, world, ever, produced, anything, tyra...\n",
      "8    [islam, answer, anything, pretend, answer, ill...\n",
      "9    [attacking, everyone, follows, religious, cult...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [wrong, isi, follow, exampl, moham, quran, exa...\n",
       "1          [good, muslim, good, despit, bad, religion]\n",
       "2    [yeah, call, care, human, life, idiot, someth,...\n",
       "3    [muslim, brain, dead, repeat, other, said, mil...\n",
       "4    [want, understand, lie, muslim, live, peac, je...\n",
       "5    [total, liar, like, pedophil, prophet, un, sol...\n",
       "6    [daesh, shia, militia, driven, religion, hatr,...\n",
       "7    [muslim, world, ever, produc, anyth, tyrant, d...\n",
       "8    [islam, answer, anyth, pretend, answer, illog,...\n",
       "9    [attack, everyon, follow, religi, cult, hate, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "twitter_df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in twitter_df['tokenized']] \n",
    "print(twitter_df['tokenized_text'].head(10))\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "twitter_df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in twitter_df['tokenized_text'] ]\n",
    "twitter_df['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[['stemmed_tokens']], twitter_df['oh_label'], stratify=twitter_df['oh_label'], random_state=42)\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "y_train = y_train.to_frame()\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.to_frame()\n",
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later\n",
    "X_train.to_csv(\"data/X_train.csv\")\n",
    "X_test.to_csv(\"data/X_test.csv\")\n",
    "y_train.to_csv(\"data/y_train.csv\")\n",
    "y_test.to_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['hi', 'wifion', 'thank', 'much', 'patient', 'address', 'comment', 'sorri', 'get', 'back', 'earlier', 'understand', 'relev', 'guidelin', 'seem', 'wp', 'undu', 'content', 'mai', 'satisfi', 'requir', 'major', 'view', 'also', 'understand', 'point', 'involv', 'parti', 'court', 'case', 'becom', 'primari', 'sourc', 'topic', 'relat', 'case', 'per', 'notw', 'exampl', 'still', 'convinc', 'bar', 'bench', 'reliabl', 'sourc', 'll', 'try', 'look', 'bit', 'see', 'rational', 'us', 'particularli', 'case', 'blp', 'somewhat', 'specul', 'question', 'suppos', 'court', 'case', 'settl', 'suffici', 'medium', 'coverag', 'would', 'lawsuit', 'notabl', 'enough', 'articl', 'would', 'subject', 'blp', 'restrict', 'anywai', 'thank', 'help']),\n",
       "       list(['serious', 'believ', 'episod', 'air', 'cancel', 'show', 'pleas', 'explain', 'known', 'provid', 'refer', 'verifi']),\n",
       "       list(['good', 'point', 'expect', 'see', 'articl', 'delet', 'bother', 'clear', 'wikipedia', 'truli', 'repres', 'forum', 'taken', 'hack', 'zealot', 'vandal', 'becom', 'adminsitr', 'larg', 'would', 'appear', 'base', 'connolli', 'bio', 'due', 'fact', 'product', 'employ', 'oblig', 'elsewher', 'fortun', 'oblig', 'expect', 'yell', 'scream', 'revert', 'control', 'ultim', 'howev', 'alter', 'fact', 'wrong', 'fundament', 'philosophi', 'new', 'fascism', 'ultim', 'die', 'form', 'issu', 'realli', 'mani', 'peopl', 'harm', 'fascist', 'us', 'global', 'weapon', 'latest', 'justif', 'increas', 'state', 'control', 'life', 'follow', 'racial', 'argument', 'nationalist', 'argument', 'worker', 'right', 'argument', 'lol', 've', 'busi', 'thing', 've', 'even', 'forgotten', 'insert', 'signatur']),\n",
       "       ..., list(['oh', 'lower', 'case', 'alreadi', 'fix']),\n",
       "       list(['ag', 'stretch', 'articl', 'run', 'room', 'ned', 'page', 'stretch']),\n",
       "       list(['sinc', 'particip', 'discuss', 'pizzl', 'hy', 'would', 'feign', 'ignor'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec_model_file = \"model/twitter_all_data.csv\" + 'word2vec_' + '.model'\n",
    "word2vec_model_file = \"model/twitter_train_data.csv\" + 'word2vec_' + '.model'\n",
    "\n",
    "stemmed_tokens = pd.Series(X_train['stemmed_tokens']).values\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 34.105093240737915\n"
     ]
    }
   ],
   "source": [
    "# Train the Word2Vec Model (Skip-gram model (sg = 1))\n",
    "start_time = time.time()\n",
    "\n",
    "w2v_model = Word2Vec(stemmed_tokens, vector_size=100,min_count = 1, workers = 4, window = 3, sg = 1)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'articl': 0,\n",
       " 'page': 1,\n",
       " 'wikipedia': 2,\n",
       " 'edit': 3,\n",
       " 'us': 4,\n",
       " 'like': 5,\n",
       " 'on': 6,\n",
       " 'fuck': 7,\n",
       " 'would': 8,\n",
       " 'pleas': 9,\n",
       " 'delet': 10,\n",
       " 'talk': 11,\n",
       " 'go': 12,\n",
       " 'thank': 13,\n",
       " 'see': 14,\n",
       " 'sourc': 15,\n",
       " 'think': 16,\n",
       " 'know': 17,\n",
       " 'make': 18,\n",
       " 'peopl': 19,\n",
       " 'also': 20,\n",
       " 'get': 21,\n",
       " 'time': 22,\n",
       " 'sai': 23,\n",
       " 'block': 24,\n",
       " 'want': 25,\n",
       " 'need': 26,\n",
       " 'remov': 27,\n",
       " 'person': 28,\n",
       " 'name': 29,\n",
       " 'imag': 30,\n",
       " 'mai': 31,\n",
       " 'user': 32,\n",
       " 'link': 33,\n",
       " 'look': 34,\n",
       " 'even': 35,\n",
       " 'suck': 36,\n",
       " 'work': 37,\n",
       " 'help': 38,\n",
       " 'inform': 39,\n",
       " 'good': 40,\n",
       " 'new': 41,\n",
       " 'well': 42,\n",
       " 'refer': 43,\n",
       " 'faggot': 44,\n",
       " 'wai': 45,\n",
       " 'comment': 46,\n",
       " 'list': 47,\n",
       " 'chang': 48,\n",
       " 'take': 49,\n",
       " 'could': 50,\n",
       " 'discuss': 51,\n",
       " 'vandal': 52,\n",
       " 'question': 53,\n",
       " 'ad': 54,\n",
       " 'thing': 55,\n",
       " 'editor': 56,\n",
       " 'point': 57,\n",
       " 'read': 58,\n",
       " 'section': 59,\n",
       " 'first': 60,\n",
       " 'right': 61,\n",
       " 'hi': 62,\n",
       " 'wp': 63,\n",
       " 'fact': 64,\n",
       " 'seem': 65,\n",
       " 'find': 66,\n",
       " 'state': 67,\n",
       " 'revert': 68,\n",
       " 'shit': 69,\n",
       " 'style': 70,\n",
       " 'try': 71,\n",
       " 'reason': 72,\n",
       " 'nigger': 73,\n",
       " 'realli': 74,\n",
       " 've': 75,\n",
       " 'mani': 76,\n",
       " 'place': 77,\n",
       " 'ask': 78,\n",
       " 'much': 79,\n",
       " 'made': 80,\n",
       " 'hate': 81,\n",
       " 'mean': 82,\n",
       " 'call': 83,\n",
       " 'stop': 84,\n",
       " 'sinc': 85,\n",
       " 'back': 86,\n",
       " 'word': 87,\n",
       " 'tag': 88,\n",
       " 'post': 89,\n",
       " 'includ': 90,\n",
       " 'wiki': 91,\n",
       " 'add': 92,\n",
       " 'creat': 93,\n",
       " 'still': 94,\n",
       " 'someon': 95,\n",
       " 'note': 96,\n",
       " 'year': 97,\n",
       " 'two': 98,\n",
       " 'polici': 99,\n",
       " 'actual': 100,\n",
       " 'content': 101,\n",
       " 'said': 102,\n",
       " 'issu': 103,\n",
       " 'come': 104,\n",
       " 'consid': 105,\n",
       " 'someth': 106,\n",
       " 'put': 107,\n",
       " 'give': 108,\n",
       " 'that': 109,\n",
       " 'dai': 110,\n",
       " 'case': 111,\n",
       " 'as': 112,\n",
       " 'die': 113,\n",
       " 'feel': 114,\n",
       " 'claim': 115,\n",
       " 'show': 116,\n",
       " 'mention': 117,\n",
       " 'contribut': 118,\n",
       " 'sure': 119,\n",
       " 'without': 120,\n",
       " 'problem': 121,\n",
       " 'admin': 122,\n",
       " 'histori': 123,\n",
       " 'let': 124,\n",
       " 'keep': 125,\n",
       " 'differ': 126,\n",
       " 'subject': 127,\n",
       " 'interest': 128,\n",
       " 'write': 129,\n",
       " 'notabl': 130,\n",
       " 'attack': 131,\n",
       " 'anoth': 132,\n",
       " 'll': 133,\n",
       " 'never': 134,\n",
       " 'free': 135,\n",
       " 'jew': 136,\n",
       " 'continu': 137,\n",
       " 'start': 138,\n",
       " 'believ': 139,\n",
       " 'request': 140,\n",
       " 'might': 141,\n",
       " 'part': 142,\n",
       " 'welcom': 143,\n",
       " 'better': 144,\n",
       " 'hope': 145,\n",
       " 'understand': 146,\n",
       " 'suggest': 147,\n",
       " 'howev': 148,\n",
       " 'done': 149,\n",
       " 'support': 150,\n",
       " 'anyth': 151,\n",
       " 'follow': 152,\n",
       " 'origin': 153,\n",
       " 'gener': 154,\n",
       " 'can': 155,\n",
       " 'view': 156,\n",
       " 'life': 157,\n",
       " 'war': 158,\n",
       " 'cunt': 159,\n",
       " 'gai': 160,\n",
       " 'site': 161,\n",
       " 'littl': 162,\n",
       " 'agre': 163,\n",
       " 'tell': 164,\n",
       " 'noth': 165,\n",
       " 'book': 166,\n",
       " 'opinion': 167,\n",
       " 'copyright': 168,\n",
       " 'regard': 169,\n",
       " 'provid': 170,\n",
       " 'rule': 171,\n",
       " 'leav': 172,\n",
       " 'color': 173,\n",
       " 'top': 174,\n",
       " 'fat': 175,\n",
       " 'check': 176,\n",
       " 'world': 177,\n",
       " 'wrong': 178,\n",
       " 'notic': 179,\n",
       " 'best': 180,\n",
       " 'current': 181,\n",
       " 'review': 182,\n",
       " 'oh': 183,\n",
       " 'must': 184,\n",
       " 'move': 185,\n",
       " 'alreadi': 186,\n",
       " 'peni': 187,\n",
       " 'term': 188,\n",
       " 'though': 189,\n",
       " 'appear': 190,\n",
       " 'messag': 191,\n",
       " 'last': 192,\n",
       " 'account': 193,\n",
       " 'explain': 194,\n",
       " 'other': 195,\n",
       " 'anyon': 196,\n",
       " 'long': 197,\n",
       " 'dick': 198,\n",
       " 'text': 199,\n",
       " 'ban': 200,\n",
       " 'reliabl': 201,\n",
       " 'love': 202,\n",
       " 'hei': 203,\n",
       " 'exampl': 204,\n",
       " 'live': 205,\n",
       " 'correct': 206,\n",
       " 'report': 207,\n",
       " 'speedi': 208,\n",
       " 'found': 209,\n",
       " 'number': 210,\n",
       " 'bad': 211,\n",
       " 'gui': 212,\n",
       " 're': 213,\n",
       " 'bla': 214,\n",
       " 'moron': 215,\n",
       " 'exist': 216,\n",
       " 'import': 217,\n",
       " 'rather': 218,\n",
       " 'thought': 219,\n",
       " 'fair': 220,\n",
       " 'got': 221,\n",
       " 'sorri': 222,\n",
       " 'probabl': 223,\n",
       " 'ip': 224,\n",
       " 'english': 225,\n",
       " 'warn': 226,\n",
       " 'templat': 227,\n",
       " 'everi': 228,\n",
       " 'non': 229,\n",
       " 'matter': 230,\n",
       " 'care': 231,\n",
       " 'lot': 232,\n",
       " 'bitch': 233,\n",
       " 'possibl': 234,\n",
       " 'great': 235,\n",
       " 'background': 236,\n",
       " 'els': 237,\n",
       " 'cite': 238,\n",
       " 'guidelin': 239,\n",
       " 'languag': 240,\n",
       " 'ye': 241,\n",
       " 'administr': 242,\n",
       " 'align': 243,\n",
       " 'group': 244,\n",
       " 'old': 245,\n",
       " 'around': 246,\n",
       " 'utc': 247,\n",
       " 'accept': 248,\n",
       " 'either': 249,\n",
       " 'enough': 250,\n",
       " 'address': 251,\n",
       " 'titl': 252,\n",
       " 'statement': 253,\n",
       " 'simpli': 254,\n",
       " 'websit': 255,\n",
       " 'idea': 256,\n",
       " 'kill': 257,\n",
       " 'materi': 258,\n",
       " 'stupid': 259,\n",
       " 'yet': 260,\n",
       " 'real': 261,\n",
       " 'complet': 262,\n",
       " 'ever': 263,\n",
       " 'least': 264,\n",
       " 'evid': 265,\n",
       " 'commun': 266,\n",
       " 'far': 267,\n",
       " 'quit': 268,\n",
       " 'relat': 269,\n",
       " 'recent': 270,\n",
       " 'hello': 271,\n",
       " 'base': 272,\n",
       " 'consensu': 273,\n",
       " 'encyclopedia': 274,\n",
       " 'date': 275,\n",
       " 'criterion': 276,\n",
       " 'etc': 277,\n",
       " 'american': 278,\n",
       " 'research': 279,\n",
       " 'fag': 280,\n",
       " 'specif': 281,\n",
       " 'happen': 282,\n",
       " 'end': 283,\n",
       " 'instead': 284,\n",
       " 'clear': 285,\n",
       " 'version': 286,\n",
       " 'mayb': 287,\n",
       " 'bit': 288,\n",
       " 'clearli': 289,\n",
       " 'nation': 290,\n",
       " 'nippl': 291,\n",
       " 'given': 292,\n",
       " 'true': 293,\n",
       " 'alwai': 294,\n",
       " 'pictur': 295,\n",
       " 'medium': 296,\n",
       " 'quot': 297,\n",
       " 'man': 298,\n",
       " 'topic': 299,\n",
       " 'violat': 300,\n",
       " 'categori': 301,\n",
       " 'improv': 302,\n",
       " 'cock': 303,\n",
       " 'posit': 304,\n",
       " 'allow': 305,\n",
       " 'countri': 306,\n",
       " 'written': 307,\n",
       " 'file': 308,\n",
       " 'author': 309,\n",
       " 'sever': 310,\n",
       " 'concern': 311,\n",
       " 'left': 312,\n",
       " 'there': 313,\n",
       " 'big': 314,\n",
       " 'perhap': 315,\n",
       " 'whether': 316,\n",
       " 'redirect': 317,\n",
       " 'sign': 318,\n",
       " 'upload': 319,\n",
       " 'dont': 320,\n",
       " 'pov': 321,\n",
       " 'present': 322,\n",
       " 'polit': 323,\n",
       " 'involv': 324,\n",
       " 'dickhead': 325,\n",
       " 'kind': 326,\n",
       " 'plai': 327,\n",
       " 'power': 328,\n",
       " 'school': 329,\n",
       " 'public': 330,\n",
       " 'answer': 331,\n",
       " 'second': 332,\n",
       " 'propos': 333,\n",
       " 'definit': 334,\n",
       " 'nigga': 335,\n",
       " 'game': 336,\n",
       " 'respons': 337,\n",
       " 'ignor': 338,\n",
       " 'line': 339,\n",
       " 'three': 340,\n",
       " 'project': 341,\n",
       " 'idiot': 342,\n",
       " 'mind': 343,\n",
       " 'critic': 344,\n",
       " 'accus': 345,\n",
       " 'le': 346,\n",
       " 'neutral': 347,\n",
       " 'wish': 348,\n",
       " 'citat': 349,\n",
       " 'argument': 350,\n",
       " 'action': 351,\n",
       " 'sentenc': 352,\n",
       " 'major': 353,\n",
       " 'cannot': 354,\n",
       " 'vertic': 355,\n",
       " 'publish': 356,\n",
       " 'border': 357,\n",
       " 'learn': 358,\n",
       " 'cours': 359,\n",
       " 'decid': 360,\n",
       " 'god': 361,\n",
       " 'disput': 362,\n",
       " 'summari': 363,\n",
       " 'whole': 364,\n",
       " 'addit': 365,\n",
       " 'anywai': 366,\n",
       " 'result': 367,\n",
       " 'common': 368,\n",
       " 'piec': 369,\n",
       " 'experi': 370,\n",
       " 'attempt': 371,\n",
       " 'main': 372,\n",
       " 'order': 373,\n",
       " 'huge': 374,\n",
       " 'wanker': 375,\n",
       " 'type': 376,\n",
       " 'lead': 377,\n",
       " 'happi': 378,\n",
       " 'entir': 379,\n",
       " 'standard': 380,\n",
       " 'seen': 381,\n",
       " 'abus': 382,\n",
       " 'detail': 383,\n",
       " 'test': 384,\n",
       " 'known': 385,\n",
       " 'act': 386,\n",
       " 'verifi': 387,\n",
       " 'ok': 388,\n",
       " 'week': 389,\n",
       " 'offici': 390,\n",
       " 'protect': 391,\n",
       " 'appreci': 392,\n",
       " 'sens': 393,\n",
       " 'requir': 394,\n",
       " 'relev': 395,\n",
       " 'past': 396,\n",
       " 'fix': 397,\n",
       " 'awai': 398,\n",
       " 'becom': 399,\n",
       " 'describ': 400,\n",
       " 'head': 401,\n",
       " 'member': 402,\n",
       " 'open': 403,\n",
       " 'parti': 404,\n",
       " 'he': 405,\n",
       " 'form': 406,\n",
       " 'side': 407,\n",
       " 'rt': 408,\n",
       " 'appropri': 409,\n",
       " 'close': 410,\n",
       " 'nomin': 411,\n",
       " 'lol': 412,\n",
       " 'nice': 413,\n",
       " 'caus': 414,\n",
       " 'compani': 415,\n",
       " 'system': 416,\n",
       " 'entri': 417,\n",
       " 'asshol': 418,\n",
       " 'friend': 419,\n",
       " 'next': 420,\n",
       " 'indic': 421,\n",
       " 'woman': 422,\n",
       " 'width': 423,\n",
       " 'everyon': 424,\n",
       " 'object': 425,\n",
       " 'stai': 426,\n",
       " 'respect': 427,\n",
       " 'unit': 428,\n",
       " 'aid': 429,\n",
       " 'info': 430,\n",
       " 'record': 431,\n",
       " 'full': 432,\n",
       " 'hand': 433,\n",
       " 'repli': 434,\n",
       " 'damn': 435,\n",
       " 'singl': 436,\n",
       " 'univers': 437,\n",
       " 'citi': 438,\n",
       " 'copi': 439,\n",
       " 'accord': 440,\n",
       " 'four': 441,\n",
       " 'middl': 442,\n",
       " 'class': 443,\n",
       " 'larg': 444,\n",
       " 'faith': 445,\n",
       " 'self': 446,\n",
       " 'wrote': 447,\n",
       " 'per': 448,\n",
       " 'eat': 449,\n",
       " 'although': 450,\n",
       " 'truth': 451,\n",
       " 'freedom': 452,\n",
       " 'film': 453,\n",
       " 'stuff': 454,\n",
       " 'wikipedian': 455,\n",
       " 'process': 456,\n",
       " 'discu': 457,\n",
       " 'run': 458,\n",
       " 'later': 459,\n",
       " 'unblock': 460,\n",
       " 'solid': 461,\n",
       " 'law': 462,\n",
       " 'paragraph': 463,\n",
       " 'deal': 464,\n",
       " 'govern': 465,\n",
       " 'promot': 466,\n",
       " 'final': 467,\n",
       " 'hard': 468,\n",
       " 'month': 469,\n",
       " 'speak': 470,\n",
       " 'abl': 471,\n",
       " 'area': 472,\n",
       " 'vote': 473,\n",
       " 'rememb': 474,\n",
       " 'anti': 475,\n",
       " 'assum': 476,\n",
       " 'meet': 477,\n",
       " 'within': 478,\n",
       " 'pretti': 479,\n",
       " 'pad': 480,\n",
       " 'search': 481,\n",
       " 'band': 482,\n",
       " 'guess': 483,\n",
       " 'especi': 484,\n",
       " 'taken': 485,\n",
       " 'human': 486,\n",
       " 'bastard': 487,\n",
       " 'everyth': 488,\n",
       " 'mother': 489,\n",
       " 'sort': 490,\n",
       " 'releas': 491,\n",
       " 'small': 492,\n",
       " 'fals': 493,\n",
       " 'forc': 494,\n",
       " 'todai': 495,\n",
       " 'whatev': 496,\n",
       " 'access': 497,\n",
       " 'certainli': 498,\n",
       " 'cover': 499,\n",
       " 'wonder': 500,\n",
       " 'noob': 501,\n",
       " 'christian': 502,\n",
       " 'we': 503,\n",
       " 'total': 504,\n",
       " 'white': 505,\n",
       " 'organ': 506,\n",
       " 'obvious': 507,\n",
       " 'sound': 508,\n",
       " 'high': 509,\n",
       " 'hell': 510,\n",
       " 'theori': 511,\n",
       " 'unless': 512,\n",
       " 'pro': 513,\n",
       " 'similar': 514,\n",
       " 'web': 515,\n",
       " 'minor': 516,\n",
       " 'reader': 517,\n",
       " 'due': 518,\n",
       " 'set': 519,\n",
       " 'fail': 520,\n",
       " 'watch': 521,\n",
       " 'controversi': 522,\n",
       " 'hour': 523,\n",
       " 'thei': 524,\n",
       " 'activ': 525,\n",
       " 'replac': 526,\n",
       " 'therefor': 527,\n",
       " 'hitler': 528,\n",
       " 'ago': 529,\n",
       " 'respond': 530,\n",
       " 'stori': 531,\n",
       " 'civil': 532,\n",
       " 'came': 533,\n",
       " 'homo': 534,\n",
       " 'descript': 535,\n",
       " 'charact': 536,\n",
       " 'shut': 537,\n",
       " 'develop': 538,\n",
       " 'futur': 539,\n",
       " 'black': 540,\n",
       " 'loser': 541,\n",
       " 'famili': 542,\n",
       " 'basic': 543,\n",
       " 'direct': 544,\n",
       " 'child': 545,\n",
       " 'cultur': 546,\n",
       " 'german': 547,\n",
       " 'confus': 548,\n",
       " 'event': 549,\n",
       " 'sandbox': 550,\n",
       " 'bullshit': 551,\n",
       " 'explan': 552,\n",
       " 'exactli': 553,\n",
       " 'alon': 554,\n",
       " 'enjoi': 555,\n",
       " 'wikiproject': 556,\n",
       " 'music': 557,\n",
       " 'pig': 558,\n",
       " 'appli': 559,\n",
       " 'stand': 560,\n",
       " 'produc': 561,\n",
       " 'disagre': 562,\n",
       " 'histor': 563,\n",
       " 'miss': 564,\n",
       " 'conflict': 565,\n",
       " 'im': 566,\n",
       " 'email': 567,\n",
       " 'figur': 568,\n",
       " 'british': 569,\n",
       " 'lie': 570,\n",
       " 'contact': 571,\n",
       " 'effect': 572,\n",
       " 'along': 573,\n",
       " 'archiv': 574,\n",
       " 'took': 575,\n",
       " 'avoid': 576,\n",
       " 'except': 577,\n",
       " 'featur': 578,\n",
       " 'studi': 579,\n",
       " 'job': 580,\n",
       " 'particular': 581,\n",
       " 'smell': 582,\n",
       " 'knowledg': 583,\n",
       " 'googl': 584,\n",
       " 'photo': 585,\n",
       " 'tri': 586,\n",
       " 'cheer': 587,\n",
       " 'disrupt': 588,\n",
       " 'desu': 589,\n",
       " 'updat': 590,\n",
       " 'death': 591,\n",
       " 'lack': 592,\n",
       " 'busi': 593,\n",
       " 'suppos': 594,\n",
       " 'statu': 595,\n",
       " 'john': 596,\n",
       " 'often': 597,\n",
       " 'error': 598,\n",
       " 'wait': 599,\n",
       " 'five': 600,\n",
       " 'valid': 601,\n",
       " 'fine': 602,\n",
       " 'turn': 603,\n",
       " 'merg': 604,\n",
       " 'level': 605,\n",
       " 'natur': 606,\n",
       " 'usernam': 607,\n",
       " 'almost': 608,\n",
       " 'prove': 609,\n",
       " 'debat': 610,\n",
       " 'muslim': 611,\n",
       " 'product': 612,\n",
       " 'awar': 613,\n",
       " 'super': 614,\n",
       " 'fucker': 615,\n",
       " 'usual': 616,\n",
       " 'log': 617,\n",
       " 'npov': 618,\n",
       " 'soon': 619,\n",
       " 'internet': 620,\n",
       " 'nonsens': 621,\n",
       " 'remain': 622,\n",
       " 'individu': 623,\n",
       " 'short': 624,\n",
       " 'yeah': 625,\n",
       " 'comput': 626,\n",
       " 'proof': 627,\n",
       " 'islam': 628,\n",
       " 'signific': 629,\n",
       " 'otherwis': 630,\n",
       " 'confirm': 631,\n",
       " 'control': 632,\n",
       " 'sock': 633,\n",
       " 'bring': 634,\n",
       " 'yourselfgo': 635,\n",
       " 'prefer': 636,\n",
       " 'attent': 637,\n",
       " 'purpos': 638,\n",
       " 'face': 639,\n",
       " 'serious': 640,\n",
       " 'told': 641,\n",
       " 'what': 642,\n",
       " 'mr': 643,\n",
       " 'document': 644,\n",
       " 'fan': 645,\n",
       " 'ball': 646,\n",
       " 'troll': 647,\n",
       " 'apolog': 648,\n",
       " 'certain': 649,\n",
       " 'nazi': 650,\n",
       " 'separ': 651,\n",
       " 'team': 652,\n",
       " 'coupl': 653,\n",
       " 'contain': 654,\n",
       " 'appar': 655,\n",
       " 'share': 656,\n",
       " 'expect': 657,\n",
       " 'jewish': 658,\n",
       " 'bot': 659,\n",
       " 'inde': 660,\n",
       " 'format': 661,\n",
       " 'sex': 662,\n",
       " 'saw': 663,\n",
       " 'simpl': 664,\n",
       " 'size': 665,\n",
       " 'church': 666,\n",
       " 'fggt': 667,\n",
       " 'murder': 668,\n",
       " 'bia': 669,\n",
       " 'boi': 670,\n",
       " 'argu': 671,\n",
       " 'baster': 672,\n",
       " 'spam': 673,\n",
       " 'licens': 674,\n",
       " 'song': 675,\n",
       " 'video': 676,\n",
       " 'space': 677,\n",
       " 'rape': 678,\n",
       " 'sometim': 679,\n",
       " 'mistak': 680,\n",
       " 'cocksuck': 681,\n",
       " 'scienc': 682,\n",
       " 'ey': 683,\n",
       " 'harass': 684,\n",
       " 'extern': 685,\n",
       " 'obviou': 686,\n",
       " 'biographi': 687,\n",
       " 'context': 688,\n",
       " 'fight': 689,\n",
       " 'oppos': 690,\n",
       " 'no': 691,\n",
       " 'accur': 692,\n",
       " 'million': 693,\n",
       " 'mexican': 694,\n",
       " 'insult': 695,\n",
       " 'greek': 696,\n",
       " 'translat': 697,\n",
       " 'thu': 698,\n",
       " 'effort': 699,\n",
       " 'variou': 700,\n",
       " 'bitchesfuck': 701,\n",
       " 'legal': 702,\n",
       " 'particip': 703,\n",
       " 'avail': 704,\n",
       " 'automat': 705,\n",
       " 'box': 706,\n",
       " 'practic': 707,\n",
       " 'third': 708,\n",
       " 'togeth': 709,\n",
       " 'manual': 710,\n",
       " 'click': 711,\n",
       " 'doubt': 712,\n",
       " 'begin': 713,\n",
       " 'went': 714,\n",
       " 'longer': 715,\n",
       " 'situat': 716,\n",
       " 'spell': 717,\n",
       " 'men': 718,\n",
       " 'seri': 719,\n",
       " 'center': 720,\n",
       " 'anonym': 721,\n",
       " 'absolut': 722,\n",
       " 'mark': 723,\n",
       " 'rest': 724,\n",
       " 'belong': 725,\n",
       " 'contest': 726,\n",
       " 'proper': 727,\n",
       " 'receiv': 728,\n",
       " 'establish': 729,\n",
       " 'afd': 730,\n",
       " 'heil': 731,\n",
       " 'repres': 732,\n",
       " 'sexual': 733,\n",
       " 'offer': 734,\n",
       " 'construct': 735,\n",
       " 'previou': 736,\n",
       " 'upon': 737,\n",
       " 'field': 738,\n",
       " 'locat': 739,\n",
       " 'bias': 740,\n",
       " 'toward': 741,\n",
       " 'intern': 742,\n",
       " 'assert': 743,\n",
       " 'ag': 744,\n",
       " 'insert': 745,\n",
       " 'king': 746,\n",
       " 'threat': 747,\n",
       " 'mere': 748,\n",
       " 'dog': 749,\n",
       " 'shall': 750,\n",
       " 'period': 751,\n",
       " 'popul': 752,\n",
       " 'limit': 753,\n",
       " 'intent': 754,\n",
       " 'seriou': 755,\n",
       " 'award': 756,\n",
       " 'tabl': 757,\n",
       " 'build': 758,\n",
       " 'associ': 759,\n",
       " 'multipl': 760,\n",
       " 'hous': 761,\n",
       " 'rational': 762,\n",
       " 'religion': 763,\n",
       " 'independ': 764,\n",
       " 'none': 765,\n",
       " 'bother': 766,\n",
       " 'episod': 767,\n",
       " 'contributor': 768,\n",
       " 'servic': 769,\n",
       " 'fire': 770,\n",
       " 'restor': 771,\n",
       " 'arab': 772,\n",
       " 'buttseck': 773,\n",
       " 'qualiti': 774,\n",
       " 'racist': 775,\n",
       " 'decis': 776,\n",
       " 'necessari': 777,\n",
       " 'outsid': 778,\n",
       " 'de': 779,\n",
       " 'deserv': 780,\n",
       " 'ga': 781,\n",
       " 'repeat': 782,\n",
       " 'blank': 783,\n",
       " 'south': 784,\n",
       " 'album': 785,\n",
       " 'heard': 786,\n",
       " 'map': 787,\n",
       " 'special': 788,\n",
       " 'tild': 789,\n",
       " 'poop': 790,\n",
       " 'belief': 791,\n",
       " 'oxymoron': 792,\n",
       " 'girl': 793,\n",
       " 'behavior': 794,\n",
       " 'hear': 795,\n",
       " 'preced': 796,\n",
       " 'hold': 797,\n",
       " 'connect': 798,\n",
       " 'robert': 799,\n",
       " 'defin': 800,\n",
       " 'okai': 801,\n",
       " 'retard': 802,\n",
       " 'data': 803,\n",
       " 'count': 804,\n",
       " 'assad': 805,\n",
       " 'realiz': 806,\n",
       " 'dumb': 807,\n",
       " 'plan': 808,\n",
       " 'return': 809,\n",
       " 'home': 810,\n",
       " 'break': 811,\n",
       " 'extrem': 812,\n",
       " 'inclus': 813,\n",
       " 'altern': 814,\n",
       " 'offic': 815,\n",
       " 'join': 816,\n",
       " 'express': 817,\n",
       " 'india': 818,\n",
       " 'design': 819,\n",
       " 'educ': 820,\n",
       " 'bunkstev': 821,\n",
       " 'joke': 822,\n",
       " 'creation': 823,\n",
       " 'press': 824,\n",
       " 'oper': 825,\n",
       " 'bollock': 826,\n",
       " 'half': 827,\n",
       " 'wast': 828,\n",
       " 'juli': 829,\n",
       " 'specifi': 830,\n",
       " 'reach': 831,\n",
       " 'kid': 832,\n",
       " 'anim': 833,\n",
       " 'region': 834,\n",
       " 'ethnic': 835,\n",
       " 'movi': 836,\n",
       " 'red': 837,\n",
       " 'worth': 838,\n",
       " 'student': 839,\n",
       " 'push': 840,\n",
       " 'step': 841,\n",
       " 'directli': 842,\n",
       " 'compar': 843,\n",
       " 'lost': 844,\n",
       " 'meant': 845,\n",
       " 'uk': 846,\n",
       " 'intend': 847,\n",
       " 'admit': 848,\n",
       " 'bodi': 849,\n",
       " 'rais': 850,\n",
       " 'court': 851,\n",
       " 'advertis': 852,\n",
       " 'social': 853,\n",
       " 'fit': 854,\n",
       " 'earli': 855,\n",
       " 'born': 856,\n",
       " 'paper': 857,\n",
       " 'tv': 858,\n",
       " 'militari': 859,\n",
       " 'unfortun': 860,\n",
       " 'expand': 861,\n",
       " 'chanc': 862,\n",
       " 'centuri': 863,\n",
       " 'onlin': 864,\n",
       " 'club': 865,\n",
       " 'gave': 866,\n",
       " 'logic': 867,\n",
       " 'consist': 868,\n",
       " 'interpret': 869,\n",
       " 'dead': 870,\n",
       " 'modern': 871,\n",
       " 'earlier': 872,\n",
       " 'di': 873,\n",
       " 'clean': 874,\n",
       " 'cut': 875,\n",
       " 'incorrect': 876,\n",
       " 'valu': 877,\n",
       " 'hanib': 878,\n",
       " 'infobox': 879,\n",
       " 'among': 880,\n",
       " 'hit': 881,\n",
       " 'north': 882,\n",
       " 'mine': 883,\n",
       " 'program': 884,\n",
       " 'alleg': 885,\n",
       " 'manag': 886,\n",
       " 'phrase': 887,\n",
       " 'constitut': 888,\n",
       " 'romnei': 889,\n",
       " 'pillar': 890,\n",
       " 'save': 891,\n",
       " 'amount': 892,\n",
       " 'concept': 893,\n",
       " 'particularli': 894,\n",
       " 'useless': 895,\n",
       " 'sockpuppet': 896,\n",
       " 'prevent': 897,\n",
       " 'march': 898,\n",
       " 'spanish': 899,\n",
       " 'normal': 900,\n",
       " 'ha': 901,\n",
       " 'atheist': 902,\n",
       " 'light': 903,\n",
       " 'referenc': 904,\n",
       " 'defend': 905,\n",
       " 'advic': 906,\n",
       " 'america': 907,\n",
       " 'primari': 908,\n",
       " 'scientif': 909,\n",
       " 'send': 910,\n",
       " 'determin': 911,\n",
       " 'reflect': 912,\n",
       " 'mitt': 913,\n",
       " 'elect': 914,\n",
       " 'em': 915,\n",
       " 'nobodi': 916,\n",
       " 'select': 917,\n",
       " 'barnstar': 918,\n",
       " 'star': 919,\n",
       " 'equal': 920,\n",
       " 'presid': 921,\n",
       " 'inappropri': 922,\n",
       " 'race': 923,\n",
       " 'russian': 924,\n",
       " 'chicken': 925,\n",
       " 'fun': 926,\n",
       " 'ahead': 927,\n",
       " 'cool': 928,\n",
       " 'indian': 929,\n",
       " 'season': 930,\n",
       " 'israel': 931,\n",
       " 'wale': 932,\n",
       " 'local': 933,\n",
       " 'engag': 934,\n",
       " 'irrelev': 935,\n",
       " 'dear': 936,\n",
       " 'poor': 937,\n",
       " 'religi': 938,\n",
       " 'behind': 939,\n",
       " 'flag': 940,\n",
       " 'french': 941,\n",
       " 'code': 942,\n",
       " 'pussi': 943,\n",
       " 'crap': 944,\n",
       " 'tutori': 945,\n",
       " 'minut': 946,\n",
       " 'font': 947,\n",
       " 'colleg': 948,\n",
       " 'tool': 949,\n",
       " 'al': 950,\n",
       " 'son': 951,\n",
       " 'physic': 952,\n",
       " 'diff': 953,\n",
       " 'air': 954,\n",
       " 'popular': 955,\n",
       " 'win': 956,\n",
       " 'listen': 957,\n",
       " 'art': 958,\n",
       " 'fack': 959,\n",
       " 'letter': 960,\n",
       " 'june': 961,\n",
       " 'drop': 962,\n",
       " 'monei': 963,\n",
       " 'moment': 964,\n",
       " 'academ': 965,\n",
       " 'wide': 966,\n",
       " 'stub': 967,\n",
       " 'unsign': 968,\n",
       " 'rate': 969,\n",
       " 'guid': 970,\n",
       " 'sent': 971,\n",
       " 'player': 972,\n",
       " 'former': 973,\n",
       " 'offens': 974,\n",
       " 'identifi': 975,\n",
       " 'cellpad': 976,\n",
       " 'august': 977,\n",
       " 'realiti': 978,\n",
       " 'board': 979,\n",
       " 'gone': 980,\n",
       " 'easi': 981,\n",
       " 'factual': 982,\n",
       " 'impli': 983,\n",
       " 'georg': 984,\n",
       " 'neither': 985,\n",
       " 'expert': 986,\n",
       " 'serv': 987,\n",
       " 'refus': 988,\n",
       " 'label': 989,\n",
       " 'pathet': 990,\n",
       " 'mail': 991,\n",
       " 'justifi': 992,\n",
       " 'movement': 993,\n",
       " 'mongo': 994,\n",
       " 'chines': 995,\n",
       " 'polic': 996,\n",
       " 'youbollock': 997,\n",
       " 'april': 998,\n",
       " 'immedi': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(w2v_model.wv.key_to_index))\n",
    "w2v_model.wv.key_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126579"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stepped_list = list(itertools.chain.from_iterable(stemmed_tokens.tolist()))\n",
    "len(set(stepped_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('religi', 0.8370547294616699), ('humanist', 0.8314415216445923), ('islam', 0.8253030776977539), ('creed', 0.8213331699371338), ('hinduism', 0.819883406162262), ('atheism', 0.8170770406723022), ('judaism', 0.8143739104270935), ('irreligion', 0.8069251775741577), ('monotheist', 0.8062925338745117), ('sikhism', 0.8054744601249695)]\n",
      "[('feminist', 0.8098196983337402), ('egalitarian', 0.7891940474510193), ('paglia', 0.7797747850418091), ('nambla', 0.7643048167228699), ('overtli', 0.7521183490753174), ('conservat', 0.7409563064575195), ('movement', 0.7394248247146606), ('persuas', 0.7275168299674988), ('paranorm', 0.7261168360710144), ('subcultur', 0.723408043384552)]\n",
      "[('other', 0.7898426651954651), ('fervent', 0.7837676405906677), ('instinct', 0.783477783203125), ('terroris', 0.7691183090209961), ('profoundli', 0.7682755589485168), ('fierc', 0.7667680978775024), ('pitchfork', 0.763868510723114), ('shun', 0.7602774500846863), ('deepak', 0.7602328658103943), ('appeas', 0.7536629438400269)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar('religion'))\n",
    "print(w2v_model.wv.most_similar('femin'))\n",
    "print(w2v_model.wv.most_similar('peopl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>wrong isi follows example mohammed quran exactly</td>\n",
       "      <td>[wrong, isi, follows, example, mohammed, quran...</td>\n",
       "      <td>[wrong, isi, follow, exampl, moham, quran, exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@SirajZarook @OdiniaInvictus @BilalIGhumman @I...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>good muslim good despite bad religion</td>\n",
       "      <td>[good, muslim, good, despite, bad, religion]</td>\n",
       "      <td>[good, muslim, good, despit, bad, religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@scamp_faridxx @AbuAlbaraaSham Yeah, it's call...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>yeah called caring human life idiot something ...</td>\n",
       "      <td>[yeah, called, caring, human, life, idiot, som...</td>\n",
       "      <td>[yeah, call, care, human, life, idiot, someth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>@Asadumarfans You are a Muslim. You are brain ...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>muslim brain dead repeat others said million time</td>\n",
       "      <td>[muslim, brain, dead, repeat, others, said, mi...</td>\n",
       "      <td>[muslim, brain, dead, repeat, other, said, mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>@harmlesstree2 @MaxBlumenthal If you want to u...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>want understand lie muslim living peace jew re...</td>\n",
       "      <td>[want, understand, lie, muslim, living, peace,...</td>\n",
       "      <td>[want, understand, lie, muslim, live, peac, je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0           0   \n",
       "1             1             1           1   \n",
       "2             2             2           2   \n",
       "3             3             3           3   \n",
       "4             4             4           4   \n",
       "\n",
       "                                                text annotation  oh_label  \\\n",
       "0  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0   \n",
       "1  @SirajZarook @OdiniaInvictus @BilalIGhumman @I...     racism       1.0   \n",
       "2  @scamp_faridxx @AbuAlbaraaSham Yeah, it's call...     racism       1.0   \n",
       "3  @Asadumarfans You are a Muslim. You are brain ...     racism       1.0   \n",
       "4  @harmlesstree2 @MaxBlumenthal If you want to u...     racism       1.0   \n",
       "\n",
       "   cat_enc ed_label_0 ed_label_1 hashtags  \\\n",
       "0        1                             []   \n",
       "1        1                             []   \n",
       "2        1                             []   \n",
       "3        1                             []   \n",
       "4        1                             []   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0   wrong isi follows example mohammed quran exactly   \n",
       "1              good muslim good despite bad religion   \n",
       "2  yeah called caring human life idiot something ...   \n",
       "3  muslim brain dead repeat others said million time   \n",
       "4  want understand lie muslim living peace jew re...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [wrong, isi, follows, example, mohammed, quran...   \n",
       "1       [good, muslim, good, despite, bad, religion]   \n",
       "2  [yeah, called, caring, human, life, idiot, som...   \n",
       "3  [muslim, brain, dead, repeat, others, said, mi...   \n",
       "4  [want, understand, lie, muslim, living, peace,...   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [wrong, isi, follow, exampl, moham, quran, exa...  \n",
       "1        [good, muslim, good, despit, bad, religion]  \n",
       "2  [yeah, call, care, human, life, idiot, someth,...  \n",
       "3  [muslim, brain, dead, repeat, other, said, mil...  \n",
       "4  [want, understand, lie, muslim, live, peac, je...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.to_csv(\"data/twitter_all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.2, Unnamed: 0.1, Unnamed: 0, text, annotation, oh_label, cat_enc, ed_label_0, ed_label_1, hashtags, tokenized, tokenized_text, stemmed_tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['oh_label'].isna()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
