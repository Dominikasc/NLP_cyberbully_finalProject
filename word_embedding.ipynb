{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus # sample text for performing tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219585</th>\n",
       "      <td>219585</td>\n",
       "      <td>219585</td>\n",
       "      <td>`   These sources don't exactly exude a sense ...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>[]</td>\n",
       "      <td>source exactly exude sense impartiality newswe...</td>\n",
       "      <td>['source', 'exactly', 'exude', 'sense', 'impar...</td>\n",
       "      <td>['sourc', 'exactli', 'exud', 'sens', 'imparti'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219586</th>\n",
       "      <td>219586</td>\n",
       "      <td>219586</td>\n",
       "      <td>The Institute for Historical Review is a pee...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[]</td>\n",
       "      <td>institute historical review peer reviewed jour...</td>\n",
       "      <td>['institute', 'historical', 'review', 'peer', ...</td>\n",
       "      <td>['institut', 'histor', 'review', 'peer', 'revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219587</th>\n",
       "      <td>219587</td>\n",
       "      <td>219587</td>\n",
       "      <td>:The way you're trying to describe it in this...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>way trying describe article serious step where...</td>\n",
       "      <td>['way', 'trying', 'describe', 'article', 'seri...</td>\n",
       "      <td>['wai', 'try', 'describ', 'articl', 'seriou', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219588</th>\n",
       "      <td>219588</td>\n",
       "      <td>219588</td>\n",
       "      <td>== Warning ==  There is clearly a protection...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>warning clearly protectionist regime going wit...</td>\n",
       "      <td>['warning', 'clearly', 'protectionist', 'regim...</td>\n",
       "      <td>['warn', 'clearli', 'protectionist', 'regim', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219589</th>\n",
       "      <td>219589</td>\n",
       "      <td>219589</td>\n",
       "      <td>Alternate option=== Is there perhaps enough ne...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>alternate option perhaps enough newsworthy inf...</td>\n",
       "      <td>['alternate', 'option', 'perhaps', 'enough', '...</td>\n",
       "      <td>['altern', 'option', 'perhap', 'enough', 'news...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  \\\n",
       "219585        219585      219585   \n",
       "219586        219586      219586   \n",
       "219587        219587      219587   \n",
       "219588        219588      219588   \n",
       "219589        219589      219589   \n",
       "\n",
       "                                                     text annotation  \\\n",
       "219585  `   These sources don't exactly exude a sense ...       none   \n",
       "219586    The Institute for Historical Review is a pee...       none   \n",
       "219587   :The way you're trying to describe it in this...       none   \n",
       "219588    == Warning ==  There is clearly a protection...       none   \n",
       "219589  Alternate option=== Is there perhaps enough ne...       none   \n",
       "\n",
       "        oh_label  cat_enc ed_label_0 ed_label_1 hashtags  \\\n",
       "219585       0.0        0   0.888889   0.111111       []   \n",
       "219586       0.0        0        0.9        0.1       []   \n",
       "219587       0.0        0        1.0        0.0       []   \n",
       "219588       0.0        0        0.8        0.2       []   \n",
       "219589       0.0        0        1.0        0.0       []   \n",
       "\n",
       "                                                tokenized  \\\n",
       "219585  source exactly exude sense impartiality newswe...   \n",
       "219586  institute historical review peer reviewed jour...   \n",
       "219587  way trying describe article serious step where...   \n",
       "219588  warning clearly protectionist regime going wit...   \n",
       "219589  alternate option perhaps enough newsworthy inf...   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "219585  ['source', 'exactly', 'exude', 'sense', 'impar...   \n",
       "219586  ['institute', 'historical', 'review', 'peer', ...   \n",
       "219587  ['way', 'trying', 'describe', 'article', 'seri...   \n",
       "219588  ['warning', 'clearly', 'protectionist', 'regim...   \n",
       "219589  ['alternate', 'option', 'perhaps', 'enough', '...   \n",
       "\n",
       "                                           stemmed_tokens  \n",
       "219585  ['sourc', 'exactli', 'exud', 'sens', 'imparti'...  \n",
       "219586  ['institut', 'histor', 'review', 'peer', 'revi...  \n",
       "219587  ['wai', 'try', 'describ', 'articl', 'seriou', ...  \n",
       "219588  ['warn', 'clearli', 'protectionist', 'regim', ...  \n",
       "219589  ['altern', 'option', 'perhap', 'enough', 'news...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing twitter dataset\n",
    "\n",
    "twitter_df = pd.read_csv('data/twitter_all_data.csv',index_col=False)\n",
    "twitter_df=twitter_df.fillna(\"\")\n",
    "\n",
    "twitter_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, Unnamed: 0, text, annotation, oh_label, cat_enc, ed_label_0, ed_label_1, hashtags, tokenized, tokenized_text, stemmed_tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all data is labelled\n",
    "twitter_df[twitter_df['oh_label']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wrong',\n",
       "  'isi',\n",
       "  'follows',\n",
       "  'example',\n",
       "  'mohammed',\n",
       "  'quran',\n",
       "  'exactly',\n",
       "  'good',\n",
       "  'muslim',\n",
       "  'good',\n",
       "  'despite',\n",
       "  'bad',\n",
       "  'religion',\n",
       "  'yeah',\n",
       "  'called',\n",
       "  'caring',\n",
       "  'human',\n",
       "  'life',\n",
       "  'idiot',\n",
       "  'something',\n",
       "  'genocidal',\n",
       "  'daesh',\n",
       "  'would',\n",
       "  'nt',\n",
       "  'understand',\n",
       "  'muslim',\n",
       "  'brain',\n",
       "  'dead',\n",
       "  'repeat',\n",
       "  'others',\n",
       "  'said',\n",
       "  'million',\n",
       "  'time',\n",
       "  'want',\n",
       "  'understand',\n",
       "  'lie',\n",
       "  'muslim',\n",
       "  'living',\n",
       "  'peace',\n",
       "  'jew',\n",
       "  'read',\n",
       "  'ibn',\n",
       "  'warraq',\n",
       "  'total',\n",
       "  'liar',\n",
       "  'like',\n",
       "  'pedophile',\n",
       "  'prophet',\n",
       "  'un',\n",
       "  'soldier',\n",
       "  'burn',\n",
       "  'people',\n",
       "  'alive',\n",
       "  'daesh',\n",
       "  'shia',\n",
       "  'militia',\n",
       "  'driven',\n",
       "  'religion',\n",
       "  'hatred',\n",
       "  'bigotry',\n",
       "  'freedom',\n",
       "  'tikrit',\n",
       "  'regardless',\n",
       "  'muslim',\n",
       "  'world',\n",
       "  'ever',\n",
       "  'produced',\n",
       "  'anything',\n",
       "  'tyrant',\n",
       "  'dictator',\n",
       "  'fascist',\n",
       "  'fanatic',\n",
       "  'would',\n",
       "  'support',\n",
       "  'islam',\n",
       "  'answer',\n",
       "  'anything',\n",
       "  'pretend',\n",
       "  'answer',\n",
       "  'illogical',\n",
       "  'delusional',\n",
       "  'superstition',\n",
       "  'attacking',\n",
       "  'everyone',\n",
       "  'follows',\n",
       "  'religious',\n",
       "  'cult',\n",
       "  'hated',\n",
       "  'murder',\n",
       "  'like',\n",
       "  'islam',\n",
       "  'liar',\n",
       "  'beheaded',\n",
       "  'jewish',\n",
       "  'men',\n",
       "  'one',\n",
       "  'day',\n",
       "  'sold',\n",
       "  'woman',\n",
       "  'child',\n",
       "  'slavery',\n",
       "  'islam',\n",
       "  'race',\n",
       "  'microbrain',\n",
       "  'death',\n",
       "  'cult',\n",
       "  'stupidist',\n",
       "  'argument',\n",
       "  'one',\n",
       "  'rationalizing',\n",
       "  'barbarity',\n",
       "  'islam',\n",
       "  'rofl',\n",
       "  'jew',\n",
       "  'used',\n",
       "  'live',\n",
       "  'arabian',\n",
       "  'peninsula',\n",
       "  'committing',\n",
       "  'genocide',\n",
       "  'rt',\n",
       "  'good',\n",
       "  'morning',\n",
       "  'pissed',\n",
       "  'american',\n",
       "  'patriot',\n",
       "  'cop',\n",
       "  'danger',\n",
       "  'religion',\n",
       "  'islam',\n",
       "  'built',\n",
       "  'apartheid',\n",
       "  'ethnic',\n",
       "  'cleansing',\n",
       "  'rt',\n",
       "  'isi',\n",
       "  'trying',\n",
       "  'stop',\n",
       "  'people',\n",
       "  'fleeing',\n",
       "  'talabyad',\n",
       "  'gre',\n",
       "  'spi',\n",
       "  'lie',\n",
       "  'tht',\n",
       "  'thy',\n",
       "  'captured',\n",
       "  'rocket',\n",
       "  'voice',\n",
       "  'hit',\n",
       "  'near',\n",
       "  'choose',\n",
       "  'define',\n",
       "  'quran',\n",
       "  'follower',\n",
       "  'extremist',\n",
       "  'care',\n",
       "  'revised',\n",
       "  'million',\n",
       "  'time',\n",
       "  'barbarity',\n",
       "  'islam',\n",
       "  'mohammed',\n",
       "  'people',\n",
       "  'murdered',\n",
       "  'disagreed',\n",
       "  'raped',\n",
       "  'woman',\n",
       "  'married',\n",
       "  'year',\n",
       "  'old',\n",
       "  'beheaded',\n",
       "  'jew',\n",
       "  'salon',\n",
       "  'try',\n",
       "  'shut',\n",
       "  'free',\n",
       "  'speech',\n",
       "  'shut',\n",
       "  'people',\n",
       "  'playing',\n",
       "  'race',\n",
       "  'card',\n",
       "  'ideaology',\n",
       "  'let',\n",
       "  'people',\n",
       "  'read',\n",
       "  'quran',\n",
       "  'see',\n",
       "  'come',\n",
       "  'hate',\n",
       "  'true',\n",
       "  'islam',\n",
       "  'corrupt',\n",
       "  'beginning',\n",
       "  'feed',\n",
       "  'little',\n",
       "  'piece',\n",
       "  'taqiyya',\n",
       "  'rt',\n",
       "  'article',\n",
       "  'islamist',\n",
       "  'welfare',\n",
       "  'paid',\n",
       "  'plot',\n",
       "  'west',\n",
       "  'demise',\n",
       "  'idiot',\n",
       "  'history',\n",
       "  'would',\n",
       "  'say',\n",
       "  'thatyou',\n",
       "  'never',\n",
       "  'studied',\n",
       "  'barbarity',\n",
       "  'muslim',\n",
       "  'entire',\n",
       "  'time',\n",
       "  'mean',\n",
       "  'send',\n",
       "  'muslim',\n",
       "  'back',\n",
       "  'middle',\n",
       "  'east',\n",
       "  'turn',\n",
       "  'europe',\n",
       "  'sewer',\n",
       "  'mohammed',\n",
       "  'first',\n",
       "  'wife',\n",
       "  'katjia',\n",
       "  'wealthy',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'lying',\n",
       "  'muslim',\n",
       "  'extermination',\n",
       "  'jew',\n",
       "  'rt',\n",
       "  'woman',\n",
       "  'subjected',\n",
       "  'brutal',\n",
       "  'abnormal',\n",
       "  'sex',\n",
       "  'act',\n",
       "  'marrying',\n",
       "  'isi',\n",
       "  'militant',\n",
       "  'big',\n",
       "  'lie',\n",
       "  'advocate',\n",
       "  'tolerance',\n",
       "  'coexistence',\n",
       "  'advocate',\n",
       "  'hatred',\n",
       "  'murder',\n",
       "  'try',\n",
       "  'get',\n",
       "  'education',\n",
       "  'repeat',\n",
       "  'lie',\n",
       "  'stupidity',\n",
       "  'get',\n",
       "  'imam',\n",
       "  'rt',\n",
       "  'people',\n",
       "  'work',\n",
       "  'hard',\n",
       "  'make',\n",
       "  'world',\n",
       "  'better',\n",
       "  'place',\n",
       "  'others',\n",
       "  'work',\n",
       "  'hard',\n",
       "  'book',\n",
       "  'drink',\n",
       "  'sex',\n",
       "  'party',\n",
       "  'next',\n",
       "  'door',\n",
       "  'pakistan',\n",
       "  'muslim',\n",
       "  'still',\n",
       "  'murder',\n",
       "  'christian',\n",
       "  'hindu',\n",
       "  'regularly',\n",
       "  'read',\n",
       "  'entire',\n",
       "  'quran',\n",
       "  'much',\n",
       "  'hadith',\n",
       "  'know',\n",
       "  'islam',\n",
       "  'muslim',\n",
       "  'curse',\n",
       "  'mankind',\n",
       "  'comparing',\n",
       "  'thousand',\n",
       "  'jewish',\n",
       "  'extremist',\n",
       "  'million',\n",
       "  'muslim',\n",
       "  'extremist',\n",
       "  'religion',\n",
       "  'ready',\n",
       "  'murder',\n",
       "  'apostate',\n",
       "  'moderate',\n",
       "  'muslim',\n",
       "  'woman',\n",
       "  'convert',\n",
       "  'islam',\n",
       "  'marry',\n",
       "  'muslim',\n",
       "  'men',\n",
       "  'men',\n",
       "  'convert',\n",
       "  'prison',\n",
       "  'actual',\n",
       "  'spirituality',\n",
       "  'buddhism',\n",
       "  'tao',\n",
       "  'zero',\n",
       "  'spirituality',\n",
       "  'islam',\n",
       "  'kiss',\n",
       "  'as',\n",
       "  'read',\n",
       "  'book',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'written',\n",
       "  'promote',\n",
       "  'fascist',\n",
       "  'murdering',\n",
       "  'genocide',\n",
       "  'humanity',\n",
       "  'another',\n",
       "  'race',\n",
       "  'baiting',\n",
       "  'idiot',\n",
       "  'brown',\n",
       "  'men',\n",
       "  'would',\n",
       "  'hate',\n",
       "  'hinduism',\n",
       "  'buddhism',\n",
       "  'tao',\n",
       "  'hate',\n",
       "  'islam',\n",
       "  'hell',\n",
       "  'palestinian',\n",
       "  'muslim',\n",
       "  'asshole',\n",
       "  'like',\n",
       "  'clearly',\n",
       "  'state',\n",
       "  'intention',\n",
       "  'religious',\n",
       "  'jihad',\n",
       "  'jew',\n",
       "  'tell',\n",
       "  'shit',\n",
       "  'sucking',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'like',\n",
       "  'recognize',\n",
       "  'history',\n",
       "  'crawled',\n",
       "  'cuntyou',\n",
       "  'think',\n",
       "  'photoshop',\n",
       "  'truth',\n",
       "  'machin',\n",
       "  'idiotic',\n",
       "  'never',\n",
       "  'million',\n",
       "  'native',\n",
       "  'american',\n",
       "  'died',\n",
       "  'disease',\n",
       "  'prophet',\n",
       "  'mohammed',\n",
       "  'started',\n",
       "  'hate',\n",
       "  'fest',\n",
       "  'jew',\n",
       "  'driving',\n",
       "  'three',\n",
       "  'jewish',\n",
       "  'tribe',\n",
       "  'medina',\n",
       "  'jew',\n",
       "  'completely',\n",
       "  'perished',\n",
       "  'much',\n",
       "  'arabian',\n",
       "  'kind',\n",
       "  'thing',\n",
       "  'prophet',\n",
       "  'mohammed',\n",
       "  'mohammed',\n",
       "  'never',\n",
       "  'looked',\n",
       "  'knowledge',\n",
       "  'believed',\n",
       "  'jinni',\n",
       "  'illiterate',\n",
       "  'believed',\n",
       "  'dog',\n",
       "  'ere',\n",
       "  'devil',\n",
       "  'meantime',\n",
       "  'palestinian',\n",
       "  'fucker',\n",
       "  'beheading',\n",
       "  'gay',\n",
       "  'honor',\n",
       "  'killing',\n",
       "  'that',\n",
       "  'muslim',\n",
       "  'react',\n",
       "  'traffic',\n",
       "  'light',\n",
       "  'turn',\n",
       "  'red',\n",
       "  'unfortunately',\n",
       "  'real',\n",
       "  'islam',\n",
       "  'defined',\n",
       "  'quran',\n",
       "  'hadith',\n",
       "  'backwards',\n",
       "  'inhuman',\n",
       "  'even',\n",
       "  'muslim',\n",
       "  'yes',\n",
       "  'muslim',\n",
       "  'bigot',\n",
       "  'murdering',\n",
       "  'christian',\n",
       "  'africa',\n",
       "  'decade',\n",
       "  'mohammed',\n",
       "  'imperialist',\n",
       "  'led',\n",
       "  'major',\n",
       "  'military',\n",
       "  'expedition',\n",
       "  'isi',\n",
       "  'imperialist',\n",
       "  'rt',\n",
       "  'simple',\n",
       "  'question',\n",
       "  'ask',\n",
       "  'complaining',\n",
       "  'muslim',\n",
       "  'etc',\n",
       "  'integrate',\n",
       "  'live',\n",
       "  'saying',\n",
       "  'islam',\n",
       "  'way',\n",
       "  'life',\n",
       "  'mean',\n",
       "  'tell',\n",
       "  'can',\n",
       "  'not',\n",
       "  'democracy',\n",
       "  'must',\n",
       "  'instead',\n",
       "  'follow',\n",
       "  'pedophile',\n",
       "  'skill',\n",
       "  'except',\n",
       "  'jihad',\n",
       "  'rt',\n",
       "  'muhammad',\n",
       "  'inventor',\n",
       "  'thighing',\n",
       "  'started',\n",
       "  'molesting',\n",
       "  'aisha',\n",
       "  'year',\n",
       "  'old',\n",
       "  'cc',\n",
       "  'absolute',\n",
       "  'truth',\n",
       "  'verify',\n",
       "  'hundred',\n",
       "  'source',\n",
       "  'anti',\n",
       "  'semitism',\n",
       "  'scripted',\n",
       "  'propaganda',\n",
       "  'truth',\n",
       "  'muslim',\n",
       "  'walking',\n",
       "  'away',\n",
       "  'islam',\n",
       "  'people',\n",
       "  'converting',\n",
       "  'education',\n",
       "  'kill',\n",
       "  'say',\n",
       "  'must',\n",
       "  'murdering',\n",
       "  'fascist',\n",
       "  'follow',\n",
       "  'book',\n",
       "  'right',\n",
       "  'living',\n",
       "  'severe',\n",
       "  'delusion',\n",
       "  'reason',\n",
       "  'mohammed',\n",
       "  'hated',\n",
       "  'jew',\n",
       "  'willing',\n",
       "  'convert',\n",
       "  'million',\n",
       "  'african',\n",
       "  'murdered',\n",
       "  'islam',\n",
       "  've',\n",
       "  'seen',\n",
       "  'microbrain',\n",
       "  'told',\n",
       "  'every',\n",
       "  'verse',\n",
       "  'tao',\n",
       "  'te',\n",
       "  'ching',\n",
       "  'superior',\n",
       "  'every',\n",
       "  'verse',\n",
       "  'quran',\n",
       "  'brain',\n",
       "  'would',\n",
       "  'revolted',\n",
       "  'islam',\n",
       "  'isi',\n",
       "  'different',\n",
       "  'max',\n",
       "  'friend',\n",
       "  'hamas',\n",
       "  'isi',\n",
       "  'throw',\n",
       "  'gay',\n",
       "  'rooftop',\n",
       "  'hamas',\n",
       "  'beheads',\n",
       "  'gay',\n",
       "  'lesson',\n",
       "  'world',\n",
       "  'learned',\n",
       "  'islam',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'brutal',\n",
       "  'dictator',\n",
       "  'replaced',\n",
       "  'brutal',\n",
       "  'islamist',\n",
       "  'that',\n",
       "  'complete',\n",
       "  'lie',\n",
       "  'murdered',\n",
       "  'unarmed',\n",
       "  'ezidi',\n",
       "  'civilian',\n",
       "  'done',\n",
       "  'nothing',\n",
       "  'enslaved',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'nothing',\n",
       "  'freeze',\n",
       "  'status',\n",
       "  'woman',\n",
       "  'century',\n",
       "  'denouncing',\n",
       "  'terrorism',\n",
       "  'violence',\n",
       "  'mean',\n",
       "  'denouncing',\n",
       "  'heart',\n",
       "  'islam',\n",
       "  'amazing',\n",
       "  'often',\n",
       "  'allah',\n",
       "  'revelation',\n",
       "  'met',\n",
       "  'mohammed',\n",
       "  'desire',\n",
       "  'one',\n",
       "  'became',\n",
       "  'suspicious',\n",
       "  'except',\n",
       "  'aisha',\n",
       "  'muslim',\n",
       "  'exterminated',\n",
       "  'christian',\n",
       "  'jew',\n",
       "  'continue',\n",
       "  'today',\n",
       "  'un',\n",
       "  'resolution',\n",
       "  'old',\n",
       "  'lie',\n",
       "  'property',\n",
       "  'right',\n",
       "  'arab',\n",
       "  'woman',\n",
       "  'islam',\n",
       "  'furthermore',\n",
       "  'many',\n",
       "  'muslim',\n",
       "  'country',\n",
       "  'even',\n",
       "  'record',\n",
       "  'system',\n",
       "  'account',\n",
       "  'murder',\n",
       "  'proving',\n",
       "  'muslim',\n",
       "  'liar',\n",
       "  'muslim',\n",
       "  'student',\n",
       "  'usc',\n",
       "  'hadith',\n",
       "  'database',\n",
       "  'muslim',\n",
       "  'virtually',\n",
       "  'killing',\n",
       "  'example',\n",
       "  'civilian',\n",
       "  'killed',\n",
       "  'afghanistan',\n",
       "  'killed',\n",
       "  'taliban',\n",
       "  'follower',\n",
       "  'religion',\n",
       "  'give',\n",
       "  'shit',\n",
       "  'prophet',\n",
       "  'religion',\n",
       "  'example',\n",
       "  'always',\n",
       "  'rt',\n",
       "  'spent',\n",
       "  'morning',\n",
       "  'board',\n",
       "  'election',\n",
       "  'getting',\n",
       "  'map',\n",
       "  'data',\n",
       "  'start',\n",
       "  'registering',\n",
       "  'every',\n",
       "  'black',\n",
       "  'person',\n",
       "  'ht',\n",
       "  'could',\n",
       "  'care',\n",
       "  'le',\n",
       "  'much',\n",
       "  'love',\n",
       "  'fraud',\n",
       "  'right',\n",
       "  'think',\n",
       "  'way',\n",
       "  'want',\n",
       "  'express',\n",
       "  'discovery',\n",
       "  'made',\n",
       "  'despite',\n",
       "  'islam',\n",
       "  'islam',\n",
       "  'contributed',\n",
       "  'nothing',\n",
       "  'mankind',\n",
       "  'year',\n",
       "  'muslim',\n",
       "  'kill',\n",
       "  'time',\n",
       "  'many',\n",
       "  'innocent',\n",
       "  'non',\n",
       "  'muslim',\n",
       "  'every',\n",
       "  'day',\n",
       "  'care',\n",
       "  'earthly',\n",
       "  'tyrant',\n",
       "  'also',\n",
       "  'egotistical',\n",
       "  'want',\n",
       "  'everyone',\n",
       "  'admire',\n",
       "  'bow',\n",
       "  'quran',\n",
       "  'writer',\n",
       "  'imagined',\n",
       "  'god',\n",
       "  'course',\n",
       "  'campaigning',\n",
       "  'destruction',\n",
       "  'israeli',\n",
       "  'racist',\n",
       "  'anti',\n",
       "  'semite',\n",
       "  'asshole',\n",
       "  'rt',\n",
       "  'islam',\n",
       "  'war',\n",
       "  'woman',\n",
       "  'continues',\n",
       "  'islam',\n",
       "  'continues',\n",
       "  'show',\n",
       "  'value',\n",
       "  'woman',\n",
       "  'factual',\n",
       "  'evidence',\n",
       "  'never',\n",
       "  'million',\n",
       "  'native',\n",
       "  'american',\n",
       "  'indian',\n",
       "  'north',\n",
       "  'american',\n",
       "  'continent',\n",
       "  'rt',\n",
       "  'uk',\n",
       "  'like',\n",
       "  'idiot',\n",
       "  'give',\n",
       "  'jihadis',\n",
       "  'rehabilitation',\n",
       "  'jordan',\n",
       "  'execute',\n",
       "  'joke',\n",
       "  'support',\n",
       "  'tyrant',\n",
       "  'islam',\n",
       "  'produce',\n",
       "  'nothing',\n",
       "  'tyrant',\n",
       "  'caliph',\n",
       "  'sultan',\n",
       "  'tyrant',\n",
       "  'muslim',\n",
       "  'robbed',\n",
       "  'wealthy',\n",
       "  'merchant',\n",
       "  'saudi',\n",
       "  'cleric',\n",
       "  'sun',\n",
       "  'revolves',\n",
       "  'around',\n",
       "  'earth',\n",
       "  'understand',\n",
       "  'madrassa',\n",
       "  'graduate',\n",
       "  'skill',\n",
       "  'jihad',\n",
       "  'islam',\n",
       "  'created',\n",
       "  'monster',\n",
       "  'islam',\n",
       "  'creating',\n",
       "  'year',\n",
       "  'allah',\n",
       "  'word',\n",
       "  'filth',\n",
       "  'hatred',\n",
       "  'phony',\n",
       "  'seen',\n",
       "  'jew',\n",
       "  'go',\n",
       "  'killing',\n",
       "  'rampage',\n",
       "  'holocaust',\n",
       "  'denier',\n",
       "  'violence',\n",
       "  'islam',\n",
       "  'answer',\n",
       "  'everything',\n",
       "  'say',\n",
       "  'drink',\n",
       "  'wine',\n",
       "  'heaven',\n",
       "  'islamic',\n",
       "  'heaven',\n",
       "  'basically',\n",
       "  'drunken',\n",
       "  'whore',\n",
       "  'house',\n",
       "  'spiritual',\n",
       "  'lol',\n",
       "  'claim',\n",
       "  'oh',\n",
       "  'insulted',\n",
       "  'religion',\n",
       "  'kill',\n",
       "  'child',\n",
       "  'grow',\n",
       "  'fairy',\n",
       "  'tale',\n",
       "  'muslim',\n",
       "  'never',\n",
       "  'grow',\n",
       "  'santa',\n",
       "  'white',\n",
       "  'entire',\n",
       "  'nation',\n",
       "  'saudi',\n",
       "  'punnished',\n",
       "  'since',\n",
       "  'allow',\n",
       "  'church',\n",
       "  'bible',\n",
       "  'tell',\n",
       "  'something',\n",
       "  'isi',\n",
       "  'mohammed',\n",
       "  'freedom',\n",
       "  'impose',\n",
       "  'religion',\n",
       "  'others',\n",
       "  'islam',\n",
       "  'yes',\n",
       "  'might',\n",
       "  'power',\n",
       "  'imperialism',\n",
       "  'zero',\n",
       "  'spirituality',\n",
       "  'soon',\n",
       "  'world',\n",
       "  'understand',\n",
       "  'see',\n",
       "  'burning',\n",
       "  'civilian',\n",
       "  'picture',\n",
       "  'even',\n",
       "  'burning',\n",
       "  'house',\n",
       "  'million',\n",
       "  'muslim',\n",
       "  'extremist',\n",
       "  'matter',\n",
       "  'desperately',\n",
       "  'race',\n",
       "  'baiting',\n",
       "  'trash',\n",
       "  'like',\n",
       "  'want',\n",
       "  'islam',\n",
       "  'race',\n",
       "  'like',\n",
       "  'said',\n",
       "  'stupid',\n",
       "  'comment',\n",
       "  'seek',\n",
       "  'hide',\n",
       "  'vileness',\n",
       "  'islam',\n",
       "  'rt',\n",
       "  'muslim',\n",
       "  'protest',\n",
       "  'complete',\n",
       "  'failure',\n",
       "  'aussie',\n",
       "  'want',\n",
       "  'islam',\n",
       "  'suit',\n",
       "  'way',\n",
       "  'life',\n",
       "  'every',\n",
       "  'jew',\n",
       "  'like',\n",
       "  'thousand',\n",
       "  'muslim',\n",
       "  'like',\n",
       "  'problem',\n",
       "  'far',\n",
       "  'knowledge',\n",
       "  'islam',\n",
       "  'mind',\n",
       "  'shrunken',\n",
       "  'islam',\n",
       "  'execution',\n",
       "  'that',\n",
       "  'one',\n",
       "  'tenth',\n",
       "  'day',\n",
       "  'worth',\n",
       "  'islamically',\n",
       "  'correct',\n",
       "  'isi',\n",
       "  'justifying',\n",
       "  'christianity',\n",
       "  'microbrain',\n",
       "  'religion',\n",
       "  'hiding',\n",
       "  'islamic',\n",
       "  'barbarity',\n",
       "  'billion',\n",
       "  'muslim',\n",
       "  'idiot',\n",
       "  'size',\n",
       "  'population',\n",
       "  'contribute',\n",
       "  'virtually',\n",
       "  'nothing',\n",
       "  'every',\n",
       "  'muslim',\n",
       "  'state',\n",
       "  'nasty',\n",
       "  'place',\n",
       "  'muslim',\n",
       "  'blumenthal',\n",
       "  'ever',\n",
       "  'written',\n",
       "  'ceaseless',\n",
       "  'muslim',\n",
       "  'violence',\n",
       "  'hindu',\n",
       "  'bangladesh',\n",
       "  'practicing',\n",
       "  'islam',\n",
       "  'vomiting',\n",
       "  'jew',\n",
       "  'christian',\n",
       "  'non',\n",
       "  'believer',\n",
       "  'check',\n",
       "  'code',\n",
       "  'umar',\n",
       "  'three',\n",
       "  'translation',\n",
       "  'quran',\n",
       "  'microbrain',\n",
       "  'know',\n",
       "  'far',\n",
       "  'islam',\n",
       "  'idiot',\n",
       "  'like',\n",
       "  'making',\n",
       "  'declaration',\n",
       "  'contact',\n",
       "  'reality',\n",
       "  'islam',\n",
       "  'inhuman',\n",
       "  'must',\n",
       "  'outlawed',\n",
       "  'rt',\n",
       "  'prophet',\n",
       "  'honor',\n",
       "  'rape',\n",
       "  'beheading',\n",
       "  'genocide',\n",
       "  'honor',\n",
       "  'muhammad',\n",
       "  'deserve',\n",
       "  'respect',\n",
       "  'quran',\n",
       "  'would',\n",
       "  'good',\n",
       "  'example',\n",
       "  'terrorism',\n",
       "  'phobia',\n",
       "  'irrational',\n",
       "  'hatred',\n",
       "  'thing',\n",
       "  'phobia',\n",
       "  'hatred',\n",
       "  'rational',\n",
       "  'islam',\n",
       "  'religion',\n",
       "  'zero',\n",
       "  'spiritual',\n",
       "  'content',\n",
       "  'tell',\n",
       "  'exactly',\n",
       "  'wipe',\n",
       "  'as',\n",
       "  'furthermore',\n",
       "  'islam',\n",
       "  'demand',\n",
       "  'woman',\n",
       "  'locked',\n",
       "  'home',\n",
       "  'covered',\n",
       "  'stop',\n",
       "  'lying',\n",
       "  ...]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "tweets = \" \".join(tw for tw in twitter_df.tokenized)\n",
    "all_sentences = nltk.sent_tokenize(tweets)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "all_words[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219580    [lead, original, research, proper, citation, g...\n",
      "219581                                 [well, done, thanks]\n",
      "219582    [talking, making, unjustified, major, change, ...\n",
      "219583    [yes, word, guci, puci, meaning, flash, flashl...\n",
      "219584    [comment, gentleman, article, provides, insigh...\n",
      "219585    [source, exactly, exude, sense, impartiality, ...\n",
      "219586    [institute, historical, review, peer, reviewed...\n",
      "219587    [way, trying, describe, article, serious, step...\n",
      "219588    [warning, clearly, protectionist, regime, goin...\n",
      "219589    [alternate, option, perhaps, enough, newsworth...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219580    [lead, origin, research, proper, citat, gamerg...\n",
       "219581                                  [well, done, thank]\n",
       "219582    [talk, make, unjustifi, major, chang, edit, re...\n",
       "219583    [ye, word, guci, puci, mean, flash, flashlight...\n",
       "219584    [comment, gentleman, articl, provid, insight, ...\n",
       "219585    [sourc, exactli, exud, sens, imparti, newsweek...\n",
       "219586    [institut, histor, review, peer, review, journ...\n",
       "219587    [wai, try, describ, articl, seriou, step, wher...\n",
       "219588    [warn, clearli, protectionist, regim, go, witc...\n",
       "219589    [altern, option, perhap, enough, newsworthi, i...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "twitter_df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in twitter_df['tokenized']] \n",
    "print(twitter_df['tokenized_text'].tail(10))\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "twitter_df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in twitter_df['tokenized_text'] ]\n",
    "twitter_df['stemmed_tokens'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(twitter_df[['stemmed_tokens']], twitter_df['oh_label'], stratify=twitter_df['oh_label'], random_state=42)\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "y_train = y_train.to_frame()\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.to_frame()\n",
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later\n",
    "X_train.to_csv(\"data/X_train.csv\")\n",
    "X_test.to_csv(\"data/X_test.csv\")\n",
    "y_train.to_csv(\"data/y_train.csv\")\n",
    "y_test.to_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['quit', 'alright', 'thank', 'pleas', 'excus', 'blunt', 'style', 'argument', 're', 'ction', 'other', 'attitud', 'noth', 'person', 'practic', 'grammat', 'would', 'cours', 'better', 'avoid', 'point', 'conflict', 'disagr', 'campi', 'pre', 'mptive', 'resolv', 'avoid', 'plural', 'opt', 'compromis', 'spell', 'pre', 'emptiv', 'instead', 'preemptiv', 'pre', 'mptive', 'dichotomi', 'respect', 'thank', 'civil']),\n",
       "       list(['wikipedia', 'censorship', 'receiv', 'zero', 'fund']),\n",
       "       list(['homophobia', 'section', 'butt', 'homophob', 'joke', 'dress', 'sens', 'high', 'pitch', 'voic']),\n",
       "       ...,\n",
       "       list(['bad', 'faith', 'troll', 'oppos', 'good', 'faith', 'troll', 'lol']),\n",
       "       list(['notabl', 'refrenc', 'need', 'link', 'site', 'charact', 'list', 'playabl', 'case', 'unlimit', 'code', 'tiger', 'colosseum', 'tiger', 'colosseum', 'upper', 'refer', 'articl', 'alreadi', 'wikipedia', 'lancer', 'els', 'would', 'want', 'see']),\n",
       "       list(['nice', 'nice', 'look', 'like', 'us', 'time', 'well', 'windsor', 'page', 'lsg'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from the model file\n",
    "word2vec_model_file = \"model/twitter_train_data.csv\" + 'word2vec_' + '.model'\n",
    "\n",
    "stemmed_tokens = pd.Series(X_train['stemmed_tokens']).values\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 29.9097900390625\n"
     ]
    }
   ],
   "source": [
    "# Train the Word2Vec Model on X train data (Skip-gram model (sg = 1))\n",
    "start_time = time.time()\n",
    "\n",
    "w2v_model = Word2Vec(stemmed_tokens, vector_size=100,min_count = 1, workers = 4, window = 3, sg = 1)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'articl': 0,\n",
       " 'page': 1,\n",
       " 'wikipedia': 2,\n",
       " 'edit': 3,\n",
       " 'us': 4,\n",
       " 'like': 5,\n",
       " 'on': 6,\n",
       " 'would': 7,\n",
       " 'pleas': 8,\n",
       " 'delet': 9,\n",
       " 'talk': 10,\n",
       " 'thank': 11,\n",
       " 'sourc': 12,\n",
       " 'see': 13,\n",
       " 'think': 14,\n",
       " 'make': 15,\n",
       " 'know': 16,\n",
       " 'go': 17,\n",
       " 'also': 18,\n",
       " 'time': 19,\n",
       " 'peopl': 20,\n",
       " 'get': 21,\n",
       " 'sai': 22,\n",
       " 'fuck': 23,\n",
       " 'block': 24,\n",
       " 'need': 25,\n",
       " 'imag': 26,\n",
       " 'mai': 27,\n",
       " 'name': 28,\n",
       " 'remov': 29,\n",
       " 'want': 30,\n",
       " 'link': 31,\n",
       " 'person': 32,\n",
       " 'user': 33,\n",
       " 'look': 34,\n",
       " 'help': 35,\n",
       " 'work': 36,\n",
       " 'inform': 37,\n",
       " 'even': 38,\n",
       " 'good': 39,\n",
       " 'new': 40,\n",
       " 'refer': 41,\n",
       " 'list': 42,\n",
       " 'well': 43,\n",
       " 'comment': 44,\n",
       " 'wai': 45,\n",
       " 'chang': 46,\n",
       " 'could': 47,\n",
       " 'discuss': 48,\n",
       " 'question': 49,\n",
       " 'ad': 50,\n",
       " 'section': 51,\n",
       " 'point': 52,\n",
       " 'editor': 53,\n",
       " 'thing': 54,\n",
       " 'take': 55,\n",
       " 'read': 56,\n",
       " 'first': 57,\n",
       " 'wp': 58,\n",
       " 'vandal': 59,\n",
       " 'fact': 60,\n",
       " 'seem': 61,\n",
       " 'right': 62,\n",
       " 'find': 63,\n",
       " 'state': 64,\n",
       " 'style': 65,\n",
       " 'reason': 66,\n",
       " 'revert': 67,\n",
       " 've': 68,\n",
       " 'place': 69,\n",
       " 'ask': 70,\n",
       " 'mani': 71,\n",
       " 'try': 72,\n",
       " 'made': 73,\n",
       " 'much': 74,\n",
       " 'realli': 75,\n",
       " 'mean': 76,\n",
       " 'hi': 77,\n",
       " 'tag': 78,\n",
       " 'includ': 79,\n",
       " 'call': 80,\n",
       " 'sinc': 81,\n",
       " 'add': 82,\n",
       " 'creat': 83,\n",
       " 'word': 84,\n",
       " 'suck': 85,\n",
       " 'note': 86,\n",
       " 'back': 87,\n",
       " 'someon': 88,\n",
       " 'post': 89,\n",
       " 'still': 90,\n",
       " 'polici': 91,\n",
       " 'two': 92,\n",
       " 'year': 93,\n",
       " 'content': 94,\n",
       " 'issu': 95,\n",
       " 'stop': 96,\n",
       " 'actual': 97,\n",
       " 'consid': 98,\n",
       " 'case': 99,\n",
       " 'someth': 100,\n",
       " 'said': 101,\n",
       " 'faggot': 102,\n",
       " 'put': 103,\n",
       " 'mention': 104,\n",
       " 'claim': 105,\n",
       " 'come': 106,\n",
       " 'show': 107,\n",
       " 'wiki': 108,\n",
       " 'feel': 109,\n",
       " 'sure': 110,\n",
       " 'contribut': 111,\n",
       " 'that': 112,\n",
       " 'dai': 113,\n",
       " 'without': 114,\n",
       " 'problem': 115,\n",
       " 'subject': 116,\n",
       " 'interest': 117,\n",
       " 'notabl': 118,\n",
       " 'differ': 119,\n",
       " 'histori': 120,\n",
       " 'give': 121,\n",
       " 'let': 122,\n",
       " 'request': 123,\n",
       " 'write': 124,\n",
       " 'anoth': 125,\n",
       " 'welcom': 126,\n",
       " 'start': 127,\n",
       " 'might': 128,\n",
       " 'continu': 129,\n",
       " 'believ': 130,\n",
       " 'part': 131,\n",
       " 'free': 132,\n",
       " 'jew': 133,\n",
       " 'keep': 134,\n",
       " 'howev': 135,\n",
       " 'never': 136,\n",
       " 'll': 137,\n",
       " 'suggest': 138,\n",
       " 'attack': 139,\n",
       " 'better': 140,\n",
       " 'support': 141,\n",
       " 'understand': 142,\n",
       " 'follow': 143,\n",
       " 'hate': 144,\n",
       " 'origin': 145,\n",
       " 'gener': 146,\n",
       " 'done': 147,\n",
       " 'hope': 148,\n",
       " 'admin': 149,\n",
       " 'view': 150,\n",
       " 'war': 151,\n",
       " 'copyright': 152,\n",
       " 'agre': 153,\n",
       " 'anyth': 154,\n",
       " 'book': 155,\n",
       " 'provid': 156,\n",
       " 'regard': 157,\n",
       " 'top': 158,\n",
       " 'color': 159,\n",
       " 'can': 160,\n",
       " 'opinion': 161,\n",
       " 'site': 162,\n",
       " 'review': 163,\n",
       " 'check': 164,\n",
       " 'notic': 165,\n",
       " 'nigger': 166,\n",
       " 'rule': 167,\n",
       " 'best': 168,\n",
       " 'current': 169,\n",
       " 'move': 170,\n",
       " 'shit': 171,\n",
       " 'term': 172,\n",
       " 'alreadi': 173,\n",
       " 'appear': 174,\n",
       " 'explain': 175,\n",
       " 'though': 176,\n",
       " 'wrong': 177,\n",
       " 'noth': 178,\n",
       " 'leav': 179,\n",
       " 'world': 180,\n",
       " 'text': 181,\n",
       " 'must': 182,\n",
       " 'last': 183,\n",
       " 'reliabl': 184,\n",
       " 'other': 185,\n",
       " 'littl': 186,\n",
       " 'speedi': 187,\n",
       " 'exampl': 188,\n",
       " 'bla': 189,\n",
       " 'long': 190,\n",
       " 'oh': 191,\n",
       " 'tell': 192,\n",
       " 'account': 193,\n",
       " 'messag': 194,\n",
       " 'number': 195,\n",
       " 'anyon': 196,\n",
       " 'found': 197,\n",
       " 'correct': 198,\n",
       " 'report': 199,\n",
       " 'import': 200,\n",
       " 'exist': 201,\n",
       " 'life': 202,\n",
       " 'fair': 203,\n",
       " 're': 204,\n",
       " 'rather': 205,\n",
       " 'templat': 206,\n",
       " 'fat': 207,\n",
       " 'english': 208,\n",
       " 'thought': 209,\n",
       " 'live': 210,\n",
       " 'non': 211,\n",
       " 'sorri': 212,\n",
       " 'possibl': 213,\n",
       " 'probabl': 214,\n",
       " 'background': 215,\n",
       " 'as': 216,\n",
       " 'guidelin': 217,\n",
       " 'lot': 218,\n",
       " 'matter': 219,\n",
       " 'care': 220,\n",
       " 'cite': 221,\n",
       " 'great': 222,\n",
       " 'languag': 223,\n",
       " 'got': 224,\n",
       " 'group': 225,\n",
       " 'ip': 226,\n",
       " 'bad': 227,\n",
       " 'align': 228,\n",
       " 'titl': 229,\n",
       " 'everi': 230,\n",
       " 'materi': 231,\n",
       " 'statement': 232,\n",
       " 'idea': 233,\n",
       " 'ye': 234,\n",
       " 'els': 235,\n",
       " 'accept': 236,\n",
       " 'administr': 237,\n",
       " 'address': 238,\n",
       " 'commun': 239,\n",
       " 'utc': 240,\n",
       " 'either': 241,\n",
       " 'simpli': 242,\n",
       " 'warn': 243,\n",
       " 'recent': 244,\n",
       " 'least': 245,\n",
       " 'evid': 246,\n",
       " 'relat': 247,\n",
       " 'enough': 248,\n",
       " 'websit': 249,\n",
       " 'criterion': 250,\n",
       " 'consensu': 251,\n",
       " 'base': 252,\n",
       " 'far': 253,\n",
       " 'date': 254,\n",
       " 'hei': 255,\n",
       " 'around': 256,\n",
       " 'ban': 257,\n",
       " 'specif': 258,\n",
       " 'love': 259,\n",
       " 'etc': 260,\n",
       " 'research': 261,\n",
       " 'complet': 262,\n",
       " 'hello': 263,\n",
       " 'gui': 264,\n",
       " 'quit': 265,\n",
       " 'version': 266,\n",
       " 'old': 267,\n",
       " 'yet': 268,\n",
       " 'end': 269,\n",
       " 'clear': 270,\n",
       " 'real': 271,\n",
       " 'encyclopedia': 272,\n",
       " 'happen': 273,\n",
       " 'nation': 274,\n",
       " 'medium': 275,\n",
       " 'given': 276,\n",
       " 'bit': 277,\n",
       " 'american': 278,\n",
       " 'instead': 279,\n",
       " 'quot': 280,\n",
       " 'categori': 281,\n",
       " 'clearli': 282,\n",
       " 'improv': 283,\n",
       " 'ever': 284,\n",
       " 'pictur': 285,\n",
       " 'topic': 286,\n",
       " 'posit': 287,\n",
       " 'alwai': 288,\n",
       " 'file': 289,\n",
       " 'mayb': 290,\n",
       " 'violat': 291,\n",
       " 'concern': 292,\n",
       " 'true': 293,\n",
       " 'written': 294,\n",
       " 'redirect': 295,\n",
       " 'upload': 296,\n",
       " 'perhap': 297,\n",
       " 'sever': 298,\n",
       " 'author': 299,\n",
       " 'whether': 300,\n",
       " 'allow': 301,\n",
       " 'there': 302,\n",
       " 'die': 303,\n",
       " 'countri': 304,\n",
       " 'present': 305,\n",
       " 'peni': 306,\n",
       " 'left': 307,\n",
       " 'pov': 308,\n",
       " 'involv': 309,\n",
       " 'propos': 310,\n",
       " 'answer': 311,\n",
       " 'public': 312,\n",
       " 'definit': 313,\n",
       " 'sign': 314,\n",
       " 'polit': 315,\n",
       " 'school': 316,\n",
       " 'second': 317,\n",
       " 'line': 318,\n",
       " 'respons': 319,\n",
       " 'project': 320,\n",
       " 'three': 321,\n",
       " 'game': 322,\n",
       " 'neutral': 323,\n",
       " 'critic': 324,\n",
       " 'kind': 325,\n",
       " 'citat': 326,\n",
       " 'sentenc': 327,\n",
       " 'man': 328,\n",
       " 'plai': 329,\n",
       " 'publish': 330,\n",
       " 'argument': 331,\n",
       " 'major': 332,\n",
       " 'gai': 333,\n",
       " 'wish': 334,\n",
       " 'le': 335,\n",
       " 'cannot': 336,\n",
       " 'border': 337,\n",
       " 'summari': 338,\n",
       " 'disput': 339,\n",
       " 'accus': 340,\n",
       " 'mind': 341,\n",
       " 'action': 342,\n",
       " 'vertic': 343,\n",
       " 'power': 344,\n",
       " 'cours': 345,\n",
       " 'result': 346,\n",
       " 'decid': 347,\n",
       " 'learn': 348,\n",
       " 'main': 349,\n",
       " 'addit': 350,\n",
       " 'experi': 351,\n",
       " 'lead': 352,\n",
       " 'common': 353,\n",
       " 'dont': 354,\n",
       " 'order': 355,\n",
       " 'detail': 356,\n",
       " 'type': 357,\n",
       " 'kill': 358,\n",
       " 'cunt': 359,\n",
       " 'standard': 360,\n",
       " 'test': 361,\n",
       " 'ignor': 362,\n",
       " 'whole': 363,\n",
       " 'verifi': 364,\n",
       " 'attempt': 365,\n",
       " 'requir': 366,\n",
       " 'appreci': 367,\n",
       " 'anywai': 368,\n",
       " 'offici': 369,\n",
       " 'entir': 370,\n",
       " 'happi': 371,\n",
       " 'relev': 372,\n",
       " 'nigga': 373,\n",
       " 'seen': 374,\n",
       " 'week': 375,\n",
       " 'rt': 376,\n",
       " 'describ': 377,\n",
       " 'big': 378,\n",
       " 'past': 379,\n",
       " 'form': 380,\n",
       " 'parti': 381,\n",
       " 'known': 382,\n",
       " 'protect': 383,\n",
       " 'sens': 384,\n",
       " 'fix': 385,\n",
       " 'appropri': 386,\n",
       " 'becom': 387,\n",
       " 'nomin': 388,\n",
       " 'compani': 389,\n",
       " 'member': 390,\n",
       " 'open': 391,\n",
       " 'ok': 392,\n",
       " 'side': 393,\n",
       " 'width': 394,\n",
       " 'indic': 395,\n",
       " 'system': 396,\n",
       " 'act': 397,\n",
       " 'close': 398,\n",
       " 'entri': 399,\n",
       " 'object': 400,\n",
       " 'unit': 401,\n",
       " 'aid': 402,\n",
       " 'dick': 403,\n",
       " 'copi': 404,\n",
       " 'accord': 405,\n",
       " 'caus': 406,\n",
       " 'record': 407,\n",
       " 'four': 408,\n",
       " 'univers': 409,\n",
       " 'citi': 410,\n",
       " 'info': 411,\n",
       " 'singl': 412,\n",
       " 'abus': 413,\n",
       " 'next': 414,\n",
       " 'god': 415,\n",
       " 'per': 416,\n",
       " 'woman': 417,\n",
       " 'respect': 418,\n",
       " 'although': 419,\n",
       " 'lol': 420,\n",
       " 'class': 421,\n",
       " 'repli': 422,\n",
       " 'stai': 423,\n",
       " 'awai': 424,\n",
       " 'film': 425,\n",
       " 'hand': 426,\n",
       " 'he': 427,\n",
       " 'freedom': 428,\n",
       " 'head': 429,\n",
       " 'process': 430,\n",
       " 'paragraph': 431,\n",
       " 'discu': 432,\n",
       " 'faith': 433,\n",
       " 'everyon': 434,\n",
       " 'larg': 435,\n",
       " 'full': 436,\n",
       " 'area': 437,\n",
       " 'govern': 438,\n",
       " 'later': 439,\n",
       " 'vote': 440,\n",
       " 'promot': 441,\n",
       " 'law': 442,\n",
       " 'middl': 443,\n",
       " 'friend': 444,\n",
       " 'abl': 445,\n",
       " 'solid': 446,\n",
       " 'assum': 447,\n",
       " 'nice': 448,\n",
       " 'meet': 449,\n",
       " 'month': 450,\n",
       " 'search': 451,\n",
       " 'rememb': 452,\n",
       " 'within': 453,\n",
       " 'pretti': 454,\n",
       " 'final': 455,\n",
       " 'releas': 456,\n",
       " 'wrote': 457,\n",
       " 'wikipedian': 458,\n",
       " 'speak': 459,\n",
       " 'stuff': 460,\n",
       " 'especi': 461,\n",
       " 'fag': 462,\n",
       " 'self': 463,\n",
       " 'organ': 464,\n",
       " 'access': 465,\n",
       " 'certainli': 466,\n",
       " 'taken': 467,\n",
       " 'cover': 468,\n",
       " 'sort': 469,\n",
       " 'band': 470,\n",
       " 'run': 471,\n",
       " 'piec': 472,\n",
       " 'todai': 473,\n",
       " 'deal': 474,\n",
       " 'truth': 475,\n",
       " 'theori': 476,\n",
       " 'hard': 477,\n",
       " 'similar': 478,\n",
       " 'christian': 479,\n",
       " 'forc': 480,\n",
       " 'due': 481,\n",
       " 'sound': 482,\n",
       " 'reader': 483,\n",
       " 'human': 484,\n",
       " 'anti': 485,\n",
       " 'unless': 486,\n",
       " 'huge': 487,\n",
       " 'we': 488,\n",
       " 'wonder': 489,\n",
       " 'web': 490,\n",
       " 'high': 491,\n",
       " 'pad': 492,\n",
       " 'everyth': 493,\n",
       " 'fals': 494,\n",
       " 'guess': 495,\n",
       " 'controversi': 496,\n",
       " 'replac': 497,\n",
       " 'unblock': 498,\n",
       " 'set': 499,\n",
       " 'descript': 500,\n",
       " 'stupid': 501,\n",
       " 'watch': 502,\n",
       " 'develop': 503,\n",
       " 'total': 504,\n",
       " 'activ': 505,\n",
       " 'therefor': 506,\n",
       " 'stori': 507,\n",
       " 'respond': 508,\n",
       " 'thei': 509,\n",
       " 'ago': 510,\n",
       " 'fail': 511,\n",
       " 'wikiproject': 512,\n",
       " 'cock': 513,\n",
       " 'obvious': 514,\n",
       " 'explan': 515,\n",
       " 'direct': 516,\n",
       " 'white': 517,\n",
       " 'futur': 518,\n",
       " 'came': 519,\n",
       " 'confus': 520,\n",
       " 'whatev': 521,\n",
       " 'moron': 522,\n",
       " 'charact': 523,\n",
       " 'event': 524,\n",
       " 'sandbox': 525,\n",
       " 'german': 526,\n",
       " 'dickhead': 527,\n",
       " 'produc': 528,\n",
       " 'hour': 529,\n",
       " 'minor': 530,\n",
       " 'appli': 531,\n",
       " 'cultur': 532,\n",
       " 'histor': 533,\n",
       " 'conflict': 534,\n",
       " 'civil': 535,\n",
       " 'small': 536,\n",
       " 'music': 537,\n",
       " 'archiv': 538,\n",
       " 'exactli': 539,\n",
       " 'disagre': 540,\n",
       " 'miss': 541,\n",
       " 'avoid': 542,\n",
       " 'basic': 543,\n",
       " 'email': 544,\n",
       " 'famili': 545,\n",
       " 'desu': 546,\n",
       " 'figur': 547,\n",
       " 'contact': 548,\n",
       " 'studi': 549,\n",
       " 'featur': 550,\n",
       " 'particular': 551,\n",
       " 'bitch': 552,\n",
       " 'effect': 553,\n",
       " 'british': 554,\n",
       " 'cheer': 555,\n",
       " 'googl': 556,\n",
       " 'updat': 557,\n",
       " 'took': 558,\n",
       " 'five': 559,\n",
       " 'merg': 560,\n",
       " 'except': 561,\n",
       " 'stand': 562,\n",
       " 'error': 563,\n",
       " 'statu': 564,\n",
       " 'photo': 565,\n",
       " 'often': 566,\n",
       " 'enjoi': 567,\n",
       " 'knowledg': 568,\n",
       " 'usernam': 569,\n",
       " 'black': 570,\n",
       " 'lack': 571,\n",
       " 'awar': 572,\n",
       " 'product': 573,\n",
       " 'child': 574,\n",
       " 'natur': 575,\n",
       " 'tri': 576,\n",
       " 'disrupt': 577,\n",
       " 'suppos': 578,\n",
       " 'log': 579,\n",
       " 'almost': 580,\n",
       " 'level': 581,\n",
       " 'debat': 582,\n",
       " 'usual': 583,\n",
       " 'valid': 584,\n",
       " 'signific': 585,\n",
       " 'npov': 586,\n",
       " 'short': 587,\n",
       " 'remain': 588,\n",
       " 'fine': 589,\n",
       " 'muslim': 590,\n",
       " 'confirm': 591,\n",
       " 'turn': 592,\n",
       " 'prefer': 593,\n",
       " 'islam': 594,\n",
       " 'document': 595,\n",
       " 'individu': 596,\n",
       " 'wait': 597,\n",
       " 'otherwis': 598,\n",
       " 'contain': 599,\n",
       " 'busi': 600,\n",
       " 'prove': 601,\n",
       " 'death': 602,\n",
       " 'purpos': 603,\n",
       " 'along': 604,\n",
       " 'nonsens': 605,\n",
       " 'soon': 606,\n",
       " 'lie': 607,\n",
       " 'separ': 608,\n",
       " 'licens': 609,\n",
       " 'format': 610,\n",
       " 'coupl': 611,\n",
       " 'certain': 612,\n",
       " 'team': 613,\n",
       " 'bring': 614,\n",
       " 'attent': 615,\n",
       " 'fan': 616,\n",
       " 'apolog': 617,\n",
       " 'control': 618,\n",
       " 'bullshit': 619,\n",
       " 'share': 620,\n",
       " 'proof': 621,\n",
       " 'bot': 622,\n",
       " 'job': 623,\n",
       " 'church': 624,\n",
       " 'saw': 625,\n",
       " 'inde': 626,\n",
       " 'scienc': 627,\n",
       " 'context': 628,\n",
       " 'appar': 629,\n",
       " 'space': 630,\n",
       " 'sometim': 631,\n",
       " 'size': 632,\n",
       " 'extern': 633,\n",
       " 'simpl': 634,\n",
       " 'biographi': 635,\n",
       " 'spam': 636,\n",
       " 'video': 637,\n",
       " 'bollock': 638,\n",
       " 'song': 639,\n",
       " 'comput': 640,\n",
       " 'mr': 641,\n",
       " 'translat': 642,\n",
       " 'sock': 643,\n",
       " 'internet': 644,\n",
       " 'oppos': 645,\n",
       " 'pro': 646,\n",
       " 'argu': 647,\n",
       " 'expect': 648,\n",
       " 'bia': 649,\n",
       " 'mistak': 650,\n",
       " 'homo': 651,\n",
       " 'told': 652,\n",
       " 'no': 653,\n",
       " 'accur': 654,\n",
       " 'thu': 655,\n",
       " 'what': 656,\n",
       " 'variou': 657,\n",
       " 'automat': 658,\n",
       " 'pig': 659,\n",
       " 'legal': 660,\n",
       " 'particip': 661,\n",
       " 'third': 662,\n",
       " 'alon': 663,\n",
       " 'avail': 664,\n",
       " 'obviou': 665,\n",
       " 'click': 666,\n",
       " 'john': 667,\n",
       " 'ball': 668,\n",
       " 'box': 669,\n",
       " 'eat': 670,\n",
       " 'effort': 671,\n",
       " 'manual': 672,\n",
       " 'greek': 673,\n",
       " 'contest': 674,\n",
       " 'begin': 675,\n",
       " 'practic': 676,\n",
       " 'center': 677,\n",
       " 'seri': 678,\n",
       " 'situat': 679,\n",
       " 'doubt': 680,\n",
       " 'afd': 681,\n",
       " 'fight': 682,\n",
       " 'establish': 683,\n",
       " 'asshol': 684,\n",
       " 'anonym': 685,\n",
       " 'nippl': 686,\n",
       " 'receiv': 687,\n",
       " 'went': 688,\n",
       " 'im': 689,\n",
       " 'longer': 690,\n",
       " 'field': 691,\n",
       " 'mark': 692,\n",
       " 'repres': 693,\n",
       " 'offer': 694,\n",
       " 'serious': 695,\n",
       " 'assert': 696,\n",
       " 'intern': 697,\n",
       " 'mother': 698,\n",
       " 'previou': 699,\n",
       " 'locat': 700,\n",
       " 'limit': 701,\n",
       " 'boi': 702,\n",
       " 'proper': 703,\n",
       " 'sexual': 704,\n",
       " 'rational': 705,\n",
       " 'million': 706,\n",
       " 'togeth': 707,\n",
       " 'tabl': 708,\n",
       " 'ey': 709,\n",
       " 'smell': 710,\n",
       " 'face': 711,\n",
       " 'yeah': 712,\n",
       " 'build': 713,\n",
       " 'period': 714,\n",
       " 'insert': 715,\n",
       " 'sex': 716,\n",
       " 'associ': 717,\n",
       " 'independ': 718,\n",
       " 'troll': 719,\n",
       " 'spell': 720,\n",
       " 'buttseck': 721,\n",
       " 'belong': 722,\n",
       " 'harass': 723,\n",
       " 'shall': 724,\n",
       " 'mere': 725,\n",
       " 'ag': 726,\n",
       " 'rest': 727,\n",
       " 'religion': 728,\n",
       " 'toward': 729,\n",
       " 'multipl': 730,\n",
       " 'intent': 731,\n",
       " 'absolut': 732,\n",
       " 'restor': 733,\n",
       " 'award': 734,\n",
       " 'ga': 735,\n",
       " 'map': 736,\n",
       " 'upon': 737,\n",
       " 'servic': 738,\n",
       " 'insult': 739,\n",
       " 'episod': 740,\n",
       " 'necessari': 741,\n",
       " 'tild': 742,\n",
       " 'nazi': 743,\n",
       " 'qualiti': 744,\n",
       " 'men': 745,\n",
       " 'contributor': 746,\n",
       " 'decis': 747,\n",
       " 'construct': 748,\n",
       " 'de': 749,\n",
       " 'bias': 750,\n",
       " 'noob': 751,\n",
       " 'hell': 752,\n",
       " 'none': 753,\n",
       " 'album': 754,\n",
       " 'south': 755,\n",
       " 'popul': 756,\n",
       " 'defin': 757,\n",
       " 'seriou': 758,\n",
       " 'repeat': 759,\n",
       " 'idiot': 760,\n",
       " 'inclus': 761,\n",
       " 'data': 762,\n",
       " 'altern': 763,\n",
       " 'connect': 764,\n",
       " 'design': 765,\n",
       " 'specifi': 766,\n",
       " 'special': 767,\n",
       " 'hous': 768,\n",
       " 'hold': 769,\n",
       " 'creation': 770,\n",
       " 'count': 771,\n",
       " 'belief': 772,\n",
       " 'outsid': 773,\n",
       " 'behavior': 774,\n",
       " 'preced': 775,\n",
       " 'plan': 776,\n",
       " 'damn': 777,\n",
       " 'reach': 778,\n",
       " 'oper': 779,\n",
       " 'press': 780,\n",
       " 'extrem': 781,\n",
       " 'hear': 782,\n",
       " 'return': 783,\n",
       " 'india': 784,\n",
       " 'arab': 785,\n",
       " 'express': 786,\n",
       " 'join': 787,\n",
       " 'realiz': 788,\n",
       " 'shut': 789,\n",
       " 'juli': 790,\n",
       " 'hitler': 791,\n",
       " 'region': 792,\n",
       " 'okai': 793,\n",
       " 'break': 794,\n",
       " 'baster': 795,\n",
       " 'home': 796,\n",
       " 'heard': 797,\n",
       " 'offic': 798,\n",
       " 'jewish': 799,\n",
       " 'mexican': 800,\n",
       " 'bother': 801,\n",
       " 'earli': 802,\n",
       " 'expand': 803,\n",
       " 'educ': 804,\n",
       " 'directli': 805,\n",
       " 'advertis': 806,\n",
       " 'intend': 807,\n",
       " 'militari': 808,\n",
       " 'rais': 809,\n",
       " 'anim': 810,\n",
       " 'student': 811,\n",
       " 'compar': 812,\n",
       " 'king': 813,\n",
       " 'club': 814,\n",
       " 'paper': 815,\n",
       " 'centuri': 816,\n",
       " 'girl': 817,\n",
       " 'blank': 818,\n",
       " 'court': 819,\n",
       " 'meant': 820,\n",
       " 'murder': 821,\n",
       " 'modern': 822,\n",
       " 'step': 823,\n",
       " 'half': 824,\n",
       " 'loser': 825,\n",
       " 'social': 826,\n",
       " 'red': 827,\n",
       " 'ethnic': 828,\n",
       " 'infobox': 829,\n",
       " 'interpret': 830,\n",
       " 'unfortun': 831,\n",
       " 'onlin': 832,\n",
       " 'consist': 833,\n",
       " 'worth': 834,\n",
       " 'born': 835,\n",
       " 'pillar': 836,\n",
       " 'deserv': 837,\n",
       " 'incorrect': 838,\n",
       " 'valu': 839,\n",
       " 'among': 840,\n",
       " 'bastard': 841,\n",
       " 'earlier': 842,\n",
       " 'chanc': 843,\n",
       " 'push': 844,\n",
       " 'movi': 845,\n",
       " 'bodi': 846,\n",
       " 'fire': 847,\n",
       " 'threat': 848,\n",
       " 'phrase': 849,\n",
       " 'program': 850,\n",
       " 'constitut': 851,\n",
       " 'particularli': 852,\n",
       " 'logic': 853,\n",
       " 'alleg': 854,\n",
       " 'march': 855,\n",
       " 'clean': 856,\n",
       " 'scientif': 857,\n",
       " 'primari': 858,\n",
       " 'rape': 859,\n",
       " 'admit': 860,\n",
       " 'determin': 861,\n",
       " 'referenc': 862,\n",
       " 'tv': 863,\n",
       " 'gave': 864,\n",
       " 'reflect': 865,\n",
       " 'concept': 866,\n",
       " 'fit': 867,\n",
       " 'uk': 868,\n",
       " 'light': 869,\n",
       " 'hit': 870,\n",
       " 'prevent': 871,\n",
       " 'north': 872,\n",
       " 'lost': 873,\n",
       " 'elect': 874,\n",
       " 'amount': 875,\n",
       " 'mine': 876,\n",
       " 'select': 877,\n",
       " 'save': 878,\n",
       " 'tutori': 879,\n",
       " 'season': 880,\n",
       " 'sockpuppet': 881,\n",
       " 'inappropri': 882,\n",
       " 'normal': 883,\n",
       " 'america': 884,\n",
       " 'advic': 885,\n",
       " 'french': 886,\n",
       " 'code': 887,\n",
       " 'cut': 888,\n",
       " 'equal': 889,\n",
       " 'local': 890,\n",
       " 'fggt': 891,\n",
       " 'star': 892,\n",
       " 'irrelev': 893,\n",
       " 'joke': 894,\n",
       " 'barnstar': 895,\n",
       " 'religi': 896,\n",
       " 'al': 897,\n",
       " 'presid': 898,\n",
       " 'flag': 899,\n",
       " 'manag': 900,\n",
       " 'cellpad': 901,\n",
       " 'art': 902,\n",
       " 'stub': 903,\n",
       " 'wast': 904,\n",
       " 'race': 905,\n",
       " 'defend': 906,\n",
       " 'israel': 907,\n",
       " 'popular': 908,\n",
       " 'physic': 909,\n",
       " 'engag': 910,\n",
       " 'fun': 911,\n",
       " 'useless': 912,\n",
       " 'wide': 913,\n",
       " 'air': 914,\n",
       " 'june': 915,\n",
       " 'russian': 916,\n",
       " 'guid': 917,\n",
       " 'identifi': 918,\n",
       " 'em': 919,\n",
       " 'letter': 920,\n",
       " 'former': 921,\n",
       " 'academ': 922,\n",
       " 'kid': 923,\n",
       " 'player': 924,\n",
       " 'rate': 925,\n",
       " 'indian': 926,\n",
       " 'sent': 927,\n",
       " 'send': 928,\n",
       " 'moment': 929,\n",
       " 'impli': 930,\n",
       " 'drop': 931,\n",
       " 'diff': 932,\n",
       " 'movement': 933,\n",
       " 'tool': 934,\n",
       " 'colleg': 935,\n",
       " 'easi': 936,\n",
       " 'neither': 937,\n",
       " 'expert': 938,\n",
       " 'januari': 939,\n",
       " 'august': 940,\n",
       " 'font': 941,\n",
       " 'poor': 942,\n",
       " 'nikko': 943,\n",
       " 'dead': 944,\n",
       " 'cool': 945,\n",
       " 'behind': 946,\n",
       " 'offens': 947,\n",
       " 'racist': 948,\n",
       " 'match': 949,\n",
       " 'board': 950,\n",
       " 'initi': 951,\n",
       " 'mail': 952,\n",
       " 'april': 953,\n",
       " 'minut': 954,\n",
       " 'justifi': 955,\n",
       " 'win': 956,\n",
       " 'nobodi': 957,\n",
       " 'unsign': 958,\n",
       " 'label': 959,\n",
       " 'factual': 960,\n",
       " 'bark': 961,\n",
       " 'gone': 962,\n",
       " 'blog': 963,\n",
       " 'incid': 964,\n",
       " 'perform': 965,\n",
       " 'immedi': 966,\n",
       " 'instanc': 967,\n",
       " 'arbitr': 968,\n",
       " 'monei': 969,\n",
       " 'revis': 970,\n",
       " 'suspect': 971,\n",
       " 'fill': 972,\n",
       " 'across': 973,\n",
       " 'dear': 974,\n",
       " 'battl': 975,\n",
       " 'highli': 976,\n",
       " 'realiti': 977,\n",
       " 'resolv': 978,\n",
       " 'late': 979,\n",
       " 'serv': 980,\n",
       " 'footbal': 981,\n",
       " 'investig': 982,\n",
       " 'easili': 983,\n",
       " 'credit': 984,\n",
       " 'bottom': 985,\n",
       " 'dog': 986,\n",
       " 'surpris': 987,\n",
       " 'introduct': 988,\n",
       " 'armi': 989,\n",
       " 'chines': 990,\n",
       " 'conclus': 991,\n",
       " 'bitchesfuck': 992,\n",
       " 'european': 993,\n",
       " 'refus': 994,\n",
       " 'deneid': 995,\n",
       " 'forward': 996,\n",
       " 'engin': 997,\n",
       " 'writer': 998,\n",
       " 'neg': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(w2v_model.wv.key_to_index))\n",
    "w2v_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125748"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stepped_list = list(itertools.chain.from_iterable(stemmed_tokens.tolist()))\n",
    "len(set(stepped_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('religi', 0.8389027714729309), ('hinduism', 0.8235896825790405), ('judaism', 0.8191343545913696), ('creed', 0.8161622881889343), ('humanist', 0.8136708736419678), ('islam', 0.8129653930664062), ('monotheist', 0.811299204826355), ('irreligion', 0.8093090057373047), ('sikhism', 0.807894229888916), ('dharmic', 0.7960187792778015)]\n",
      "[('feminist', 0.8195350170135498), ('conservat', 0.781833291053772), ('paglia', 0.7771574854850769), ('gymnast', 0.7735275626182556), ('egalitarian', 0.7586634755134583), ('heterosexu', 0.7565826773643494), ('nuditi', 0.7550978064537048), ('nambla', 0.7531779408454895), ('intercours', 0.7511581778526306), ('vibrant', 0.7492883801460266)]\n",
      "[('other', 0.803199827671051), ('instinct', 0.7902354001998901), ('ingrain', 0.7806429862976074), ('werent', 0.7800484299659729), ('profoundli', 0.7774443626403809), ('wouldnt', 0.774388313293457), ('resent', 0.7698257565498352), ('becuas', 0.7697687745094299), ('mingl', 0.7695815563201904), ('sudhan', 0.7665708661079407)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.most_similar('religion'))\n",
    "print(w2v_model.wv.most_similar('femin'))\n",
    "print(w2v_model.wv.most_similar('peopl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.to_csv(\"data/twitter_all_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
