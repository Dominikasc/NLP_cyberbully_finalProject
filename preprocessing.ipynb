{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge 5 twitter datasets and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus # sample text for performing tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all datasets\n",
    "\n",
    "racism_df = pd.read_csv('data/twitter_racism_parsed_dataset.csv',index_col=False)\n",
    "general_df = pd.read_csv('data/twitter_parsed_dataset.csv',index_col=False)\n",
    "sexism_df = pd.read_csv('data/twitter_sexism_parsed_dataset.csv',index_col=False)\n",
    "toxity_df = pd.read_csv('data/toxicity_parsed_dataset.csv',index_col=False)\n",
    "aggression_df = pd.read_csv('data/aggression_parsed_dataset.csv',index_col=False)\n",
    "attack_df = pd.read_csv('data/attack_parsed_dataset.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14881"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sexism_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14878"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sexism_df = sexism_df[sexism_df['oh_label'].notna()]\n",
    "\n",
    "len(sexism_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               Text  ed_label_0  \\\n",
       "0      0  `- This is not ``creative``.  Those are the di...    1.000000   \n",
       "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
       "2      2    True or false, the situation as of March 200...    1.000000   \n",
       "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
       "4      4               This page will need disambiguation.     1.000000   \n",
       "\n",
       "   ed_label_1  oh_label Annotation  \n",
       "0    0.000000         0       none  \n",
       "1    0.000000         0       none  \n",
       "2    0.000000         0       none  \n",
       "3    0.444444         0       none  \n",
       "4    0.000000         0       none  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synchronize format for combination\n",
    "\n",
    "toxity_df['Annotation'] = np.where(toxity_df['oh_label']!= 0, 'toxity', 'none')\n",
    "aggression_df['Annotation'] = np.where(aggression_df['oh_label']!= 0, 'aggression', 'none')\n",
    "attack_df['Annotation'] = np.where(attack_df['oh_label']!= 0, 'attack', 'none')\n",
    "\n",
    "attack_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_encoder(df,enc):\n",
    "    df['Cat_enc'] = np.where(df['Annotation']!= 'none', str(enc), str(0))\n",
    "    return df\n",
    "\n",
    "racism_df = annotation_encoder(racism_df,1)\n",
    "sexism_df = annotation_encoder(sexism_df,2)\n",
    "toxity_df = annotation_encoder(toxity_df,3)\n",
    "aggression_df = annotation_encoder(aggression_df,4)\n",
    "attack_df = annotation_encoder(attack_df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Cat_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>#mkr No No No No No No</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \\\n",
       "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0   \n",
       "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0   \n",
       "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0   \n",
       "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0   \n",
       "4                             #mkr No No No No No No       none       0.0   \n",
       "\n",
       "  Cat_enc  \n",
       "0       0  \n",
       "1       0  \n",
       "2       2  \n",
       "3       1  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_df['Cat_enc'] = np.where(general_df['Annotation']== 'racism', str(1), str(0))\n",
    "general_df['Cat_enc'] = np.where(general_df['Annotation']== 'sexism', str(2), general_df['Cat_enc'])\n",
    "general_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets\n",
    "#del(twitter_df)\n",
    "twitterList =[]\n",
    "twitterList.extend(value for name, value in locals().items() if name.endswith('_df'))\n",
    "\n",
    "twitter_df = pd.concat(twitterList, ignore_index=True)\n",
    "twitter_df.head()\n",
    "twitter_df = twitter_df[twitter_df['oh_label'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df[['Text','Annotation','oh_label','Cat_enc','ed_label_0','ed_label_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213140\n",
      "219590\n",
      "244118\n",
      "244118\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicates\n",
    "\n",
    "print(len(twitter_df['Text'].unique()))\n",
    "print(len(twitter_df.groupby(['Text', 'oh_label']).first()))\n",
    "print(len(twitter_df.groupby(['Text', 'oh_label','Annotation']).first()))\n",
    "print(len(twitter_df))\n",
    "\n",
    "# Drop duplicates with same content, label and annotation, assuming that ambiguous annotation / label can be learned in multiple categories\n",
    "#twitter_df.groupby(['Text', 'oh_label','Annotation']).first().head()\n",
    "twitter_df = twitter_df.drop_duplicates(\n",
    "  subset = ['Text', 'oh_label'],\n",
    "  keep = 'last').reset_index(drop = True) # use last because those have label confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.columns= twitter_df.columns.str.lower()\n",
    "twitter_df[twitter_df['text'] == \"\"] \n",
    "\n",
    "#twitter_df.dropna(subset=['text'])\n",
    "twitter_df = twitter_df.dropna(how='all')\n",
    "twitter_df\n",
    "\n",
    "twitter_df['text']=twitter_df['text'].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter_df.replace('&amp;','&', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Hashtags into a column\n",
    "twitter_df['hashtags'] = twitter_df.text.apply(lambda x: [x for x in x.split(\" \") if x.startswith(\"#\")])\n",
    "\n",
    "# replace all hashtags, ampersands, and character references with no space\n",
    "twitter_df.hashtags = twitter_df.hashtags.apply(lambda x: re.sub(r'[#@]+', '', str(x)))\n",
    "\n",
    "twitter_df.head(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list:\n",
    "mystopwords = set(stopwords.words('english'))\n",
    "\n",
    "# create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text & remove twitter accounts\n",
    "    tknzr = TweetTokenizer(strip_handles=True)\n",
    "    tokens = tknzr.tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in mystopwords]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # remove urls\n",
    "    #processed_text = processed_text.replace('^(http|https)://', '')\n",
    "    processed_text = re.sub(r'(http|https)://[\\S]+', '',processed_text)\n",
    "\n",
    "    # remove numbers\n",
    "    processed_text = re.sub(r'(?<!\\S)(?=.)(0|([1-9](\\d*|\\d{0,2}(,\\d{3})*)))?(\\.\\d*[1-9])?(?!\\S)', '',processed_text)\n",
    "\n",
    "    # remove special characters\n",
    "    processed_text = re.sub(r'[#@&][\\S]+', '',processed_text)\n",
    "    processed_text = re.sub(r'[#@&$“”\".,’]', '',processed_text)\n",
    "    processed_text = re.sub(r'\\b\\d[\\S]+', '',processed_text)\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text) # spaces\n",
    "    processed_text = re.sub(r'\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b', ' ', processed_text) # single letters\n",
    "    \n",
    "    processed_text =\" \".join(processed_text.split())\n",
    "    processed_text =\" \".join([w for w in processed_text.split() if len(w)>1])\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# apply the function df\n",
    "\n",
    "twitter_df['tokenized'] = twitter_df['text'].apply(preprocess_text)\n",
    "twitter_df.head(n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed file\n",
    "twitter_df.to_csv(\"data/twitter_all_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>cat_enc</th>\n",
       "      <th>ed_label_0</th>\n",
       "      <th>ed_label_1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, annotation, oh_label, cat_enc, ed_label_0, ed_label_1, hashtags, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df[twitter_df['oh_label'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
